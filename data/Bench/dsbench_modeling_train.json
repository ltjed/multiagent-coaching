{"prompt": "Description  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. \n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. \n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. \n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. \n\nThe dataset used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features. \n\nGood luck and have fun! \n\nGetting Started  \nCheck out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target. \n\nSubmission File  \nFor each row in the test set, you must predict the probability of a binary target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:  \nid,target  \n5,0.5  \n6,0.1  \n8,0.9  \netc.\n\nDataset Description  \nFor this competition, you will be predicting a binary target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat18 are categorical, and the feature columns cont0 - cont10 are continuous.\n\nFiles  \n- train.csv - the training data with the target column  \n- test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)  \n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-mar-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-mar-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-mar-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-mar-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-mar-2021"}}
{"prompt": "Description\n\nSynthetically-Generated Datasets\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict the probability for the target variable defects. The file should contain a header and have the following format:\n\nid, defects  \n101763, 0.5  \n101764, 0.5  \n101765, 0.5  \netc.\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n\ntrain.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)  \ntest.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e23", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e23/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e23/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e23", "url": "https://www.kaggle.com/competitions/playground-series-s3e23"}}
{"prompt": "Description  \nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting\u2019s even more painful when you know you\u2019re a good driver. It doesn\u2019t seem fair that you have to pay so much if you\u2019ve been cautious on the road for years. Porto Seguro, one of Brazil\u2019s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company\u2019s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. \nIn this competition, you\u2019re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they\u2019re looking to Kaggle\u2019s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.\n\nEvaluation  \nScoring Metric  \nSubmissions are evaluated using the Normalized Gini Coefficient. During scoring, observations are sorted from the largest to the smallest predictions. Predictions are only used for ordering observations; therefore, the relative magnitude of the predictions is not used during scoring. The scoring algorithm then compares the cumulative proportion of positive class observations to a theoretical uniform proportion. \nThe Gini Coefficient ranges from approximately 0 for random guessing to approximately 0.5 for a perfect score. The theoretical maximum for the discrete calculation is (1 - frac_pos) / 2.\nThe Normalized Gini Coefficient adjusts the score by the theoretical maximum so that the maximum score is 1. \nThe code to calculate the Normalized Gini Coefficient in a number of different languages can be found in this forum thread. \n\nSubmission File  \nFor each id in the test set, you must predict a probability of an insurance claim in the target column. The file should contain a header and have the following format: \n\n```\nid,target \n0,0.1 \n1,0.9 \n2,1.0 \netc.\n```\n\nDataset Description\n\nData Description  \nIn this competition, you will predict the probability that an auto insurance policyholder files a claim. In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target column signifies whether or not a claim was filed for that policyholder.\n\nFile descriptions\n\n- `train.csv` contains the training data, where each row corresponds to a policyholder, and the target column signifies that a claim was filed.\n- `test.csv` contains the test data.\n- `sample_submission.csv` is the submission file showing the correct format.", "answer": null, "metadata": {"task_id": "porto-seguro-safe-driver-prediction", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/porto-seguro-safe-driver-prediction/train.csv", "test_file": "./data/DSBench/data/data_resplit/porto-seguro-safe-driver-prediction/test.csv", "answer_dir": "./data/DSBench/data/answers/porto-seguro-safe-driver-prediction", "url": "https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction"}}
{"prompt": "Description\n\n\ud83d\udc4b\ud83d\udef3\ufe0f Ahoy, welcome to Kaggle! You\u2019re in the right place. This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you\u2019re ready to start competing, click on the \"Join Competition\" button to create an account and gain access to the competition data. Then check out Alexis Cook\u2019s Titanic Tutorial that walks you through step by step how to make your first submission!\n\nThe Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (i.e. name, age, gender, socio-economic class, etc).\n\nRecommended Tutorial\n\nWe highly recommend Alexis Cook\u2019s Titanic Tutorial that walks you through making your very first submission step by step and this starter notebook to get started.\n\nOverview of How Kaggle\u2019s Competitions Work\n\nJoin the Competition\n\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n\nGet to Work\n\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n\nMake a Submission\n\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n\nCheck the Leaderboard\n\nSee how your model ranks against other Kagglers on our leaderboard.\n\nImprove Your Score\n\nCheck out the discussion forum to find lots of tutorials and insights from other competitors.\n\nKaggle Lingo Video\n\nYou may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatman\u2019s video on Kaggle Lingo to get up to speed!\n\nWhat Data Will I Use in This Competition?\n\nIn this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv. Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe test.csv dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes. Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\n\nCheck out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\nHow to Submit your Prediction to Kaggle\n\nOnce you\u2019re ready to make a submission and get on the leaderboard:\n\nClick on the \u201cSubmit Predictions\u201d button\n\nUpload a CSV file in the submission file format. You\u2019re able to submit 10 submissions a day.\n\nSubmission File Format:\n\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\n\nGot it! I\u2019m ready to get started. Where do I get help if I need it?\n\nFor Competition Help: Titanic Discussion Forum\nTechnical Help: Kaggle Contact Us Page\n\nKaggle doesn\u2019t have a dedicated support team so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\nA Last Word on Kaggle Notebooks\n\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable Jupyter Notebooks environment with free GPUs and a huge repository of community published data & code. In every competition, you\u2019ll find many Notebooks shared with incredible insights. It\u2019s an invaluable resource worth becoming familiar with. Check out this competition\u2019s Notebooks here.\n\n\ud83c\udfc3\u200d\u2640\ufe0fReady to Compete? Join the Competition Here!\n\nEvaluation\n\nGoal\n\nIt is your job to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable.\n\nMetric\n\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\nSubmission File Format\n\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\n\nPassengerId,Survived\n892,0\n893,1\n894,0\nEtc.\n\nYou can download an example submission file (gender_submission.csv) on the Data page.\n\nDataset Description\n\nOverview\n\nThe data has been split into two groups:\ntraining set (train.csv)\ntest set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\nData Dictionary\n\nVariable Definition Key\n\nsurvival: Survival (0 = No, 1 = Yes)\npclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\nsex: Sex\nAge: Age in years\nsibsp: # of siblings / spouses aboard the Titanic\nparch: # of parents / children aboard the Titanic\nticket: Ticket number\nfare: Passenger fare\ncabin: Cabin number\nembarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n\nVariable Notes\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children traveled only with a nanny, therefore parch=0 for them.", "answer": null, "metadata": {"task_id": "titanic", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/titanic/train.csv", "test_file": "./data/DSBench/data/data_resplit/titanic/test.csv", "answer_dir": "./data/DSBench/data/answers/titanic", "url": "https://www.kaggle.com/competitions/titanic"}}
{"prompt": "Description\n\n\ud83d\udce3 Recommended Competition\nWe highly recommend Titanic - Machine Learning from Disaster to get familiar with the basics of machine learning and Kaggle competitions.\n\nWelcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination\u2014the torrid 55 Cancri E\u2014the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship\u2019s damaged computer system.\n\nHelp save them and change history!\n\n\ud83d\udca1 Getting Started Notebook\nTo get started quickly, feel free to take advantage of this starter notebook.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nAcknowledgments\nPhotos by Joel Filipe, Richard Gatley and ActionVance on Unsplash.\n\nEvaluation\n\nMetric\nSubmissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.\n\nSubmission Format\nThe submission format for the competition is a csv file with the following format:\n```\nPassengerId, Transported\n0013_01, False\n0018_01, False\n0019_01, False\n0021_01, False\netc.\n```\n\nDataset Description\n\nIn this competition, your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n\nFile and Data Field Descriptions\n\ntrain.csv\n- Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n  - PassengerId: A unique Id for each passenger. Each Id takes the form `gggg_pp` where `gggg` indicates a group the passenger is travelling with and `pp` is their number within the group. People in a group are often family members, but not always.\n  - HomePlanet: The planet the passenger departed from, typically their planet of permanent residence.\n  - CryoSleep: Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n  - Cabin: The cabin number where the passenger is staying. Takes the form `deck/num/side`, where `side` can be either `P` for Port or `S` for Starboard.\n  - Destination: The planet the passenger will be debarking to.\n  - Age: The age of the passenger.\n  - VIP: Whether the passenger has paid for special VIP service during the voyage.\n  - RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n  - Name: The first and last names of the passenger.\n  - Transported: Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\ntest.csv\n- Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n\nsample_submission.csv\n- A submission file in the correct format.\n  - PassengerId: Id for each passenger in the test set.\n  - Transported: The target. For each passenger, predict either True or False.", "answer": null, "metadata": {"task_id": "spaceship-titanic", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/spaceship-titanic/train.csv", "test_file": "./data/DSBench/data/data_resplit/spaceship-titanic/test.csv", "answer_dir": "./data/DSBench/data/answers/spaceship-titanic", "url": "https://www.kaggle.com/competitions/spaceship-titanic"}}
{"prompt": "Evaluation  \nSubmissions will be evaluated using Mean Absolute Error (MAE), where each x_i represents the predicted target, y_i represents the ground truth, and n is the number of rows in the test set.\n\nSubmission File  \nFor each id in the test set, you must predict the target yield. The file should contain a header and have the following format:  \n```\nid,yield  \n15289,6025.194  \n15290,1256.223  \n15291,357.44  \netc.\n```\n\nDataset Description  \n**NOTE:** You can now create your own synthetic versions of this dataset by forking and running this notebook. The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wild Blueberry Yield Prediction Dataset. (Since this is Playground 3.14, it seems like we need a Blueberry Pie joke here?) Feature distributions are close to, but not exactly the same as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles  \n- `train.csv` - the training dataset; yield is the target  \n- `test.csv` - the test dataset; your objective is to predict the yield given the other features  \n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e14", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e14/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e14/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e14", "url": "https://www.kaggle.com/competitions/playground-series-s3e14"}}
{"prompt": "Description:\n\nAccording to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception. The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner\u2026 and often before they\u00b4ve even realized they need the service.\n\nIn their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\n\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\n\nEvaluation:\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error. The RMSLE is calculated as:\n\\[\n\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2 }\n\\]\nWhere:\n\\(\\epsilon\\) is the RMSLE value (score),\n\\(n\\) is the total number of observations in the (public/private) data set,\n\\(p_i\\) is your prediction of target, and\n\\(a_i\\) is the actual target for \\(i\\).\n\\(\\log(x)\\) is the natural logarithm of \\(x\\).\n\nSubmission File:\n\nFor every row in the test.csv, submission files should contain two columns: ID and target. The ID corresponds to the column of that ID in the test.tsv. The file should contain a header and have the following format:\n```\nID,target\n000137c73,5944923.322036332\n00021489f,5944923.322036332\n0004d7953,5944923.322036332\netc.\n```\n\nDataset Description:\n\nYou are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. The task is to predict the value of the target column in the test set.\n\nFile descriptions:\n- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "santander-value-prediction-challenge", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/santander-value-prediction-challenge/train.csv", "test_file": "./data/DSBench/data/data_resplit/santander-value-prediction-challenge/test.csv", "answer_dir": "./data/DSBench/data/answers/santander-value-prediction-challenge", "url": "https://www.kaggle.com/competitions/santander-value-prediction-challenge"}}
{"prompt": "Description\n\nGet started on this competition through Kaggle Scripts.\n\nBike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able to rent a bike from one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n\nThe data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed are explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n\nAcknowledgements\n\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:\nFanaee-T, Hadi, and Gama, Joao, \"Event labeling combining ensemble detectors and background knowledge,\" Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.\n\nEvaluation\n\nSubmissions are evaluated on the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as: \n\n\\[ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2} \\]\n\nWhere:\n- \\(n\\) is the number of hours in the test set\n- \\(p_i\\) is your predicted count\n- \\(a_i\\) is the actual count\n- \\(\\log(x)\\) is the natural logarithm \n\nSubmission Format\n\nYour submission file must have a header and should be structured in the following format:\n```\ndatetime,count\n2011-01-20 00:00:00,0\n2011-01-20 01:00:00,0\n2011-01-20 02:00:00,0\n...\n...\n```\n\nDataset Description\n\nSee, fork, and run a random forest benchmark model through Kaggle Scripts. You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n\nData Fields\n- `datetime`: hourly date + timestamp\n- `season`: 1 = spring, 2 = summer, 3 = fall, 4 = winter\n- `holiday`: whether the day is considered a holiday\n- `workingday`: whether the day is neither a weekend nor holiday\n- `weather`:\n  1. Clear, Few clouds, Partly cloudy, Partly cloudy\n  2. Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n  3. Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n  4. Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- `temp`: temperature in Celsius\n- `atemp`: \"feels like\" temperature in Celsius\n- `humidity`: relative humidity\n- `windspeed`: wind speed\n- `casual`: number of non-registered user rentals initiated\n- `registered`: number of registered user rentals initiated\n- `count`: number of total rentals", "answer": null, "metadata": {"task_id": "bike-sharing-demand", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/bike-sharing-demand/train.csv", "test_file": "./data/DSBench/data/data_resplit/bike-sharing-demand/test.csv", "answer_dir": "./data/DSBench/data/answers/bike-sharing-demand", "url": "https://www.kaggle.com/competitions/bike-sharing-demand"}}
{"prompt": "Description\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. Many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nWith the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nRoot Mean Squared Error (RMSE)\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\\[ \\textrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} \\]\n\nwhere \\( \\hat{y}_i \\) is the predicted value and \\( y_i \\) is the original value for each instance \\( i \\).\n\nSubmission File\n\nFor each id in the test set, you must predict the value for the target price. The file should contain a header and have the following format:\n\nid,price\n22709,200689.01\n22710,398870.92\n22711,1111145.11\netc.\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Paris Housing Price Prediction. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- train.csv - the training dataset; price is the target\n- test.csv - the test dataset; your objective is to predict price\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e6", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e6/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e6/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e6", "url": "https://www.kaggle.com/competitions/playground-series-s3e6"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! \n\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. \n\n\ud83d\udca1 Getting Started Notebook To get started quickly, feel free to take advantage of this starter notebook. \n\nSynthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation Submissions are evaluated on the area under the ROC curve between the predicted probability and the ground truth for each target, and the final score is the average of the individual AUCs of each predicted column. \n\nSubmission File For each id in the test set, you must predict the value for the targets EC1 and EC2. The file should contain a header and have the following format: \n```\nid,EC1,EC2 \n14838,0.22,0.71 \n14839,0.78,0.43 \n14840,0.53,0.11 \netc.\n```\n\nDataset Description The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Multi-label Classification of enzyme substrates. This dataset only uses a subset of features from the original (the features that had the most signal). Feature distributions are close to, but not exactly the same as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. \n\nNote: For this challenge, you are given 6 features in the training data but only asked to predict the first two features (EC1 and EC2).\n\nFiles:\n- `train.csv` - the training dataset; [EC1 - EC6] are the (binary) targets, although you are only asked to predict EC1 and EC2.\n- `test.csv` - the test dataset; your objective is to predict the probability of the two targets EC1 and EC2.\n- `sample_submission.csv` - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "playground-series-s3e18", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e18/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e18/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e18", "url": "https://www.kaggle.com/competitions/playground-series-s3e18"}}
{"prompt": "Evaluation\n\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that O_i,j corresponds to the number of Ids i (actual) that received a predicted value j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:\n\\[ w_{i,j} = \\frac{(i-j)^2}{(N-1)^2} \\]\n\nAn N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values. This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated as:\n\\[ \\kappa = 1 - \\frac{\\sum_{i,j} w_{i,j}O_{i,j}}{\\sum_{i,j} w_{i,j}E_{i,j}} \\]\n\nSubmission File\n\nFor each Id in the test set, you must predict the value for the target quality. The file should contain a header and have the following format:\n```\nId,quality\n2056,5\n2057,7\n2058,3\netc.\n```\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Wine Quality dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n\n- train.csv - the training dataset; quality is the target (ordinal, integer)\n- test.csv - the test dataset; your objective is to predict quality\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e5", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e5/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e5/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e5", "url": "https://www.kaggle.com/competitions/playground-series-s3e5"}}
{"prompt": "Description\n\nGoal of the Competition\n\nThe goal of this competition is to assess the language proficiency of 8th-12th grade English Language Learners (ELLs). Utilizing a dataset of essays written by ELLs will help to develop proficiency models that better support all students. Your work will help ELLs receive more accurate feedback on their language development and expedite the grading cycle for teachers. These outcomes could enable ELLs to receive more appropriate learning tasks that will help them improve their English language proficiency.\n\nContext\n\nWriting is a foundational skill. Sadly, it's one few students are able to hone, often because writing tasks are infrequently assigned in school. A rapidly growing student population, students learning English as a second language, known as English Language Learners (ELLs), are especially affected by the lack of practice. While automated feedback tools make it easier for teachers to assign more writing tasks, they are not designed with ELLs in mind. Existing tools are unable to provide feedback based on the language proficiency of the student, resulting in a final evaluation that may be skewed against the learner. Data science may be able to improve automated feedback tools to better support the unique needs of these learners.\n\nCompetition host\n\nVanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus\u2014an accredited arboretum\u2014complete with athletic facilities and state-of-the-art laboratories. Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters discoveries that have global impact. Vanderbilt and co-host, The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good. Vanderbilt and The Learning Agency Lab have partnered together to offer data scientists the opportunity to support ELLs using data science skills in machine learning, natural language processing, and educational data analytics. You can improve automated feedback tools for ELLs by sensitizing them to language proficiency. The resulting tools could serve teachers by alleviating the grading burden and support ELLs by ensuring their work is evaluated within the context of their current language level.\n\nAcknowledgments\n\nVanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.\n\nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation\n\nSubmissions are scored using MCRMSE, mean columnwise root mean squared error:\n\\[ \\textrm{MCRMSE} = \\frac{1}{N_{t}}\\sum_{j=1}^{N_{t}}\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{ij} - \\hat{y}_{ij})^2} \\]\n\nwhere \\( N_t \\) is the number of scored ground truth target columns, and \\( y \\) and \\( \\hat{y} \\) are the actual and predicted values, respectively.\n\nSubmission File\n\nFor each text_id in the test set, you must predict a value for each of the six analytic measures (described on the Data page). The file should contain a header and have the following format:\n\ntext_id,cohesion,syntax,vocabulary,phraseology,grammar,conventions  \n0000C359D63E,3.0,3.0,3.0,3.0,3.0,3.0  \n000BAD50D026,3.0,3.0,3.0,3.0,3.0,3.0  \n00367BB2546B,3.0,3.0,3.0,3.0,3.0,3.0  \n003969F4EDB6,3.0,3.0,3.0,3.0,3.0,3.0  \n...\n\nDataset Description\n\nThe dataset presented here (the ELLIPSE corpus) comprises argumentative essays written by 8th-12th grade English Language Learners (ELLs). The essays have been scored according to six analytic measures: cohesion, syntax, vocabulary, phraseology, grammar, and conventions. Each measure represents a component of proficiency in essay writing, with greater scores corresponding to greater proficiency in that measure. The scores range from 1.0 to 5.0 in increments of 0.5. Your task is to predict the score of each of the six measures for the essays given in the test set. Some of these essays have appeared in the datasets for the Feedback Prize - Evaluating Student Writing and Feedback Prize - Predicting Effective Arguments competitions. You are welcome to make use of these earlier datasets in this competition.\n\nFile and Field Information\n\n- train.csv - The training set, comprising the full_text of each essay, identified by a unique text_id. The essays are also given a score for each of the six analytic measures above: cohesion, etc. These analytic measures comprise the target for the competition.\n- test.csv - For the test data we give only the full_text of an essay together with its text_id.\n- sample_submission.csv - A submission file in the correct format. See the Evaluation page for details.\n\nPlease note that this is a Code Competition. We give a few sample essays in test.csv to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. The full test set comprises about 2700 essays.", "answer": null, "metadata": {"task_id": "feedback-prize-english-language-learning", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/feedback-prize-english-language-learning/train.csv", "test_file": "./data/DSBench/data/data_resplit/feedback-prize-english-language-learning/test.csv", "answer_dir": "./data/DSBench/data/answers/feedback-prize-english-language-learning", "url": "https://www.kaggle.com/competitions/feedback-prize-english-language-learning"}}
{"prompt": "Evaluation  \nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error. The RMSLE is calculated as:\n\n\\[ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2 } \\]\n\nwhere:\n- \\( n \\) is the total number of observations in the test set,\n- \\( \\hat{y}_i \\) is the predicted value of the target for instance \\( i \\),\n- \\( y_i \\) is the actual value of the target for instance \\( i \\), and\n- \\( \\log \\) is the natural logarithm.\n\nSubmission File  \nFor each id row in the test set, you must predict the target, Rings. The file should contain a header and have the following format:\n\n```\nid,Rings\n90615,10\n90616,10\n90617,10\netc.\n```\n\nDataset Description  \nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Abalone dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles:\n- `train.csv` - the training dataset; Rings is the integer target.\n- `test.csv` - the test dataset; your objective is to predict the value of Rings for each row.\n- `sample_submission.csv` - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "playground-series-s4e4", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s4e4/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s4e4/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s4e4", "url": "https://www.kaggle.com/competitions/playground-series-s4e4"}}
{"prompt": "Description\n\nNOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook.\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!\n\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts.\n\nPlease feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict the probability of target (likelihood of the presence of a kidney stone). The file should contain a header and have the following format:\n\n```\nid,target\n414,0.5\n415,0.1\n416,0.9\netc.\n```\n\nDataset Description\n\nNOTE:\nYou can now create your own synthetic versions of this dataset by forking and running this notebook.\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Kidney Stone Prediction based on Urine Analysis dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles:\n- train.csv - the training dataset; target is the likelihood of a kidney stone being present\n- test.csv - the test dataset; your objective is to predict the probability of target\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e12", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e12/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e12/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e12", "url": "https://www.kaggle.com/competitions/playground-series-s3e12"}}
{"prompt": "Description\n\nThe May edition of the 2022 Tabular Playground series binary classification problem includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.\n\nAbout the Tabular Playground Series\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new to their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.\n\nThe goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nGetting Started\n\nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nWe've also built a starter notebook for you that uses TensorFlow Decision Forests, a TensorFlow library that matches the power of XGBoost with a friendly, straightforward user interface.\n\nGood luck and have fun!\n\nAcknowledgments\n\nPhoto by Clarisse Croset on Unsplash.\n\nEvaluation\n\nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:\n\n```\nid, target\n900000, 0.65\n900001, 0.97\n900002, 0.02\netc.\n```\n\nDataset Description\n\nFor this challenge, you are given (simulated) manufacturing control data and are tasked to predict whether the machine is in state 0 or state 1. The data has various feature interactions that may be important in determining the machine state. Good luck!\n\nFiles\n\n- train.csv: the training data, which includes normalized continuous data and categorical data\n- test.csv: the test set; your task is to predict the binary target variable which represents the state of a manufacturing process\n- sample_submission.csv: a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-may-2022", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-may-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-may-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-may-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-may-2022"}}
{"prompt": "Description \n\nNOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook.  \n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nTo start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nRoot Mean Squared Error (RMSE)\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n\\[\\textrm{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 }\\]\n\nwhere \\(\\hat{y}_i\\) is the predicted value and \\(y_i\\) is the original value for each instance \\(i\\).\n\nSubmission File\n\nFor each id in the test set, you must predict the value for the target MedHouseVal. The file should contain a header and have the following format:  \n\n```\nid,MedHouseVal\n37137,2.01\n37138,0.92\n37139,1.11\netc.\n```\n\nDataset Description\n\nNOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook.\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the California Housing Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- train.csv - the training dataset; MedHouseVal is the target\n- test.csv - the test dataset; your objective is to predict MedHouseVal\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e1", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e1/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e1/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e1", "url": "https://www.kaggle.com/competitions/playground-series-s3e1"}}
{"prompt": "Evaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File  \nFor each id in the test set, you must predict the probability for the target variable smoking. The file should contain a header and have the following format:\n\n```\nid,smoking\n159256,0.5\n159257,0.5\n159258,0.5\netc.\n```\n\nDataset Description  \nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles  \n- train.csv: the training dataset; smoking is the binary target  \n- test.csv: the test dataset; your objective is to predict the probability of positive smoking  \n- sample_submission.csv: a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e24", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e24/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e24/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e24", "url": "https://www.kaggle.com/competitions/playground-series-s3e24"}}
{"prompt": "Description  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. \n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. \n\nThe goal of these competitions is to provide a fun and approachable tabular dataset for anyone. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. \n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. \n\nThe dataset used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features. \n\nGood luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation  \nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target. \n\nSubmission File  \nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:  \nid,target  \n100000,0.5  \n100001,0.9  \n100002,0.1  \netc.\n\nDataset Description  \nFor this competition, you will be predicting a binary target based on a number of feature columns given in the data. The columns are a mix of scaled continuous features and binary features. The data is synthetically generated by a GAN that was trained on real-world molecular response data.\n\nFiles\n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-oct-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-oct-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-oct-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-oct-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-oct-2021"}}
{"prompt": "Description\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.\n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nThe dataset used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly private test labels?\n\nGood luck and have fun!\n\nGetting Started\nCheck out the original Titanic competition which walks you through how to build various models.\n\nFor more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation\nGoal\nYour task is to predict whether or not a passenger survived the sinking of the Synthanic (a synthetic, much larger dataset based on the actual Titanic dataset). For each PassengerId row in the test set, you must predict a 0 or 1 value for the Survived target.\n\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\nSubmission File\nYou should submit a csv file with exactly 100,000 rows plus a header row. Your submission will show an error if you have extra columns or extra rows. The file should have exactly 2 columns:\n- PassengerId (sorted in any order)\n- Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n\nYou can download an example submission file (sample_submission.csv) on the Data page:\n```\nPassengerId,Survived\n100000,0\n100001,1\n100002,0\netc.\n```\n\nDataset Description\nOverview\nThe dataset used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly unseen data?\n\nThe data has been split into two groups:\n- training set (train.csv)\n- test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Synthanic.\n\nData Dictionary\n| Variable   | Definition                                | Key                                           |\n|------------|-------------------------------------------|-----------------------------------------------|\n| survival   | Survival                                  | 0 = No, 1 = Yes                               |\n| pclass     | Ticket class                              | 1 = 1st, 2 = 2nd, 3 = 3rd                     |\n| sex        | Sex                                       |                                               |\n| Age        | Age in years                              |                                               |\n| sibsp      | # of siblings/spouses aboard the Titanic  |                                               |\n| parch      | # of parents/children aboard the Titanic  |                                               |\n| ticket     | Ticket number                             |                                               |\n| fare       | Passenger fare                            |                                               |\n| cabin      | Cabin number                              |                                               |\n| embarked   | Port of Embarkation                       | C = Cherbourg, Q = Queenstown, S = Southampton|\n\nVariable Notes\n- pclass: A proxy for socio-economic status (SES)\n  - 1st = Upper\n  - 2nd = Middle\n  - 3rd = Lower\n\n- age: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5\n\n- sibsp: The dataset defines family relations in this way...\n  - Sibling = brother, sister, stepbrother, stepsister\n  - Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n- parch: The dataset defines family relations in this way...\n  - Parent = mother, father\n  - Child = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.", "answer": null, "metadata": {"task_id": "tabular-playground-series-apr-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-apr-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-apr-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-apr-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-apr-2021"}}
{"prompt": "Description  \nFor the March edition of the 2022 Tabular Playground Series, you're challenged to forecast twelve hours of traffic flow in a U.S. metropolis. The time series in this dataset are labeled with both location coordinates and a direction of travel\u2014a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network. Which model will prevail? The venerable linear regression? The deservedly-popular ensemble of decision trees? Or maybe a cutting-edge graph neural network? We can't wait to see!\n\nAbout the Tabular Playground Series  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nEvaluation  \nSubmissions are evaluated on the mean absolute error between predicted and actual congestion values for each time period in the test set.\n\nSubmission File  \nFor each row_id in the test set, you should predict a congestion measurement. The file should contain a header and have the following format:  \nrow_id,congestion  \n140140,0.0  \n140141,0.0  \n140142,0.0  \n...  \nThe congestion target has integer values from 0 to 100, but your predictions may be any floating-point number.\n\nDataset Description  \nIn this competition, you'll forecast twelve hours of traffic flow in a major U.S. metropolitan area. Time, space, and directional features give you the chance to model interactions across a network of roadways.\n\nFiles and Field Descriptions  \n**train.csv**  \n- the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.  \n**row_id**  \n- a unique identifier for this instance  \n**time**  \n- the 20-minute period in which each measurement was taken  \n**x**  \n- the east-west midpoint coordinate of the roadway  \n**y**  \n- the north-south midpoint coordinate of the roadway  \n**direction**  \n- the direction of travel of the roadway. EB indicates \"eastbound\" travel, for example, while SW indicates a \"southwest\" direction of travel.  \n**congestion**  \n- congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.  \n\n**test.csv**  \n- the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-09-30.  \n\n**sample_submission.csv**  \n- a sample submission file in the correct format  \n\nSource  \nThis dataset was derived from the Chicago Traffic Tracker - Historical Congestion Estimates dataset.", "answer": null, "metadata": {"task_id": "tabular-playground-series-mar-2022", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-mar-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-mar-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-mar-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-mar-2022"}}
{"prompt": "Description\nWe've heard your feedback from the 2021 Tabular Playground Series, and now Kaggle needs your help going forward in 2022! There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build forecasting models to help us decide. Help us figure out whether KaggleMart or KaggleRama should become the official Kaggle outlet!\n\nAbout the Tabular Playground Series\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nEvaluation\nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.\n\nSubmission File\nFor each row_id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:\nrow_id, num_sold\n26298, 100\n26299, 100\n26300, 100\netc.\n\nDataset Description\nFor this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches. Good luck!\n\nFiles\ntrain.csv - the training set, which includes the sales data for each date-country-store-item combination.\ntest.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\nsample_submission.csv - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "tabular-playground-series-jan-2022", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-jan-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-jan-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-jan-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-jan-2022"}}
{"prompt": "Description  \nThis week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there.  \nThis is week 2 of Kaggle's COVID-19 forecasting series, following the Week 1 competition. This is the 2nd of at least 4 competitions we plan to launch in this series.\n\nBackground  \nThe White House Office of Science and Technology Policy (OSTP) pulled together a coalition of research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from the National Academies of Sciences, Engineering, and Medicine (NASEM) and the World Health Organization (WHO).\n\nThe Challenge  \nKaggle is launching a companion COVID-19 forecasting challenge to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19.  \nYou are encouraged to pull in, curate, and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your findings in a notebook.  \nAs the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).  \nWe have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.\n\nCompanies and Organizations  \nThere is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.\n\nAcknowledgements  \nJHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.  \nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation  \nPublic and Private Leaderboard  \nTo have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after the competition launch. Only use data prior to 2020-03-19 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.  \nPublic Leaderboard Period: 2020-03-19 - 2020-04-01  \nPrivate Leaderboard Period: 2020-04-02 - 2020-04-30  \nEvaluation  \nSubmissions are evaluated using the column-wise root mean squared logarithmic error (RMSLE).  \nThe RMSLE for a single column is calculated as:\n\n\\[\n\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2 }\n\\]\n\nwhere:  \n\\(n\\) is the total number of observations  \n\\(p_i\\) is your prediction  \n\\(a_i\\) is the actual value  \n\\(\\log(x)\\) is the natural logarithm of \\(x\\)  \nThe final score is the mean of the RMSLE over all columns (in this case, 2).\n\nSubmission File  \nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.  \nFor each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:\n\n```\nForecastId,ConfirmedCases,Fatalities\n1,10,0\n2,10,0\n3,10,0\n```\netc.  \nYou will get the ForecastId for the corresponding date and location from the test.csv file.\n\nDataset Description  \nIn this challenge, you will be predicting the cumulative number of confirmed COVID-19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates.  \nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.  \n\nFiles  \n- train.csv - the training data (you are encouraged to join in many more useful external datasets)\n- test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on the last 28 days of predicted data.\n- submission.csv - a sample submission in the correct format; again, predictions should be cumulative.\n\nData Source  \nThis evaluation data for this competition comes from Johns Hopkins CSSE, which is uninvolved in the competition.  \nSee their README for a description of how the data was collected. They are currently updating the data daily.", "answer": null, "metadata": {"task_id": "covid19-global-forecasting-week-2", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-2/train.csv", "test_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-2/test.csv", "answer_dir": "./data/DSBench/data/answers/covid19-global-forecasting-week-2", "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-2"}}
{"prompt": "Description\n\nThe malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways. With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.\n\nAs one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences. Can you help protect more than one billion machines from damage BEFORE it happens?\n\nAcknowledgements\n\nThis competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.\n\nMicrosoft contacts:\n- Rob McCann (Robert.McCann@microsoft.com)\n- Christian Seifert (chriseif@microsoft.com)\n- Susan Higgs (Susan.Higgs@microsoft.com)\n- Matt Duncan (Matthew.Duncan@microsoft.com)\n\nNortheastern University contact:\n- Mansour Ahmadi (m.ahmadi@northeastern.edu)\n\nGeorgia Tech contacts:\n- Brendan Saltaformaggio (brendan@ece.gatech.edu)\n- Taesoo Kim (taesoo@gatech.edu)\n\nEvaluation\n\nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed label.\n\nSubmission File\n\nFor each MachineIdentifier in the test set, you must predict a probability for the HasDetections column. The file should contain a header and have the following format:\n```\nMachineIdentifier, HasDetections\n1, 0.5\n6, 0.5\n14, 0.5\netc.\n```\n\nDataset Description\n\nThe goal of this competition is to predict a Windows machine\u2019s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.\n\nEach row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.\n\nThe sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross-validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers\u2019 machines in the wild; it has been sampled to include a much larger proportion of malware machines.\n\nColumns\n\nUnavailable or self-documenting column names are marked with an \"NA\".\n\n- MachineIdentifier - Individual machine ID\n- ProductName - Defender state information e.g. win8defender\n- EngineVersion - Defender state information e.g. 1.1.12603.0\n- AppVersion - Defender state information e.g. 4.9.10586.0\n- AvSigVersion - Defender state information e.g. 1.217.1014.0\n- IsBeta - Defender state information e.g. false\n- RtpStateBitfield - NA\n- IsSxsPassiveMode - NA\n- DefaultBrowsersIdentifier - ID for the machine's default browser\n- AVProductStatesIdentifier - ID for the specific configuration of a user's antivirus software\n- AVProductsInstalled - NA\n- AVProductsEnabled - NA\n- HasTpm - True if the machine has TPM\n- CountryIdentifier - ID for the country the machine is located in\n- CityIdentifier - ID for the city the machine is located in\n- OrganizationIdentifier - ID for the organization the machine belongs in, organization ID is mapped to both specific companies and broad industries\n- GeoNameIdentifier - ID for the geographic region a machine is located in\n- LocaleEnglishNameIdentifier - English name of Locale ID of the current user\n- Platform - Calculates platform name (of OS-related properties and processor property)\n- Processor - This is the process architecture of the installed operating system\n- OsVer - Version of the current operating system\n- OsBuild - Build of the current operating system\n- OsSuite - Product suite mask for the current operating system\n- OsPlatformSubRelease - Returns the OS Platform sub-release (Windows Vista, Windows 7, Windows 8, TH1, TH2)\n- OsBuildLab - Build lab that generated the current OS. Example: 9600.17630.amd64fre.winblue_r7.150109-2022\n- SkuEdition - The goal of this feature is to use the Product Type defined in the MSDN to map to a 'SKU-Edition' name that is useful in population reporting. The valid Product Types are defined in %sdxroot%\\data\\windowseditions.xml. This API has been used since Vista and Server 2008, so there are many Product Types that do not apply to Windows 10. The 'SKU-Edition' is a string value that is in one of three classes of results. The design must hand each class.\n- IsProtected - This is a calculated field derived from the Spynet Report's AV Products field. Returns:\n  - a. TRUE if there is at least one active and up-to-date antivirus product running on this machine.\n  - b. FALSE if there is no active AV product on this machine, or if the AV is active but is not receiving the latest updates.\n  - c. null if there are no Anti Virus Products in the report.\n  - Returns: Whether a machine is protected.\n- AutoSampleOptIn - This is the SubmitSamplesConsent value passed in from the service, available on CAMP 9+\n- PuaMode - Pua Enabled mode from the service\n- SMode - This field is set to true when the device is known to be in 'S Mode', as in Windows 10 S mode, where only Microsoft Store apps can be installed\n- IeVerIdentifier - NA\n- SmartScreen - This is the SmartScreen enabled string value from registry. This is obtained by checking in order, HKLM\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\SmartScreenEnabled and HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\SmartScreenEnabled. If the value exists but is blank, the value \"ExistsNotSet\" is sent in telemetry.\n- Firewall - This attribute is true (1) for Windows 8.1 and above if Windows firewall is enabled, as reported by the service.\n- UacLuaenable - This attribute reports whether or not the \"administrator in Admin Approval Mode\" user type is disabled or enabled in UAC. The value reported is obtained by reading the regkey HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\EnableLUA.\n- Census_MDC2FormFactor - A grouping based on a combination of Device Census level hardware characteristics. The logic used to define Form Factor is rooted in business and industry standards and aligns with how people think about their device. (Examples: Smartphone, Small Tablet, All in One, Convertible\u2026)\n- Census_DeviceFamily - AKA DeviceClass. Indicates the type of device that an edition of the OS is intended for. Example values: Windows.Desktop, Windows.Mobile, and iOS.Phone\n- Census_OEMNameIdentifier - NA\n- Census_OEMModelIdentifier - NA\n- Census_ProcessorCoreCount - Number of logical cores in the processor\n- Census_ProcessorManufacturerIdentifier - NA\n- Census_ProcessorModelIdentifier - NA\n- Census_ProcessorClass - A classification of processors into high/medium/low. Initially used for Pricing Level SKU. No longer maintained and updated.\n- Census_PrimaryDiskTotalCapacity - Amount of disk space on the primary disk of the machine in MB\n- Census_PrimaryDiskTypeName - Friendly name of Primary Disk Type - HDD or SSD\n- Census_SystemVolumeTotalCapacity - The size of the partition that the System volume is installed on in MB\n- Census_HasOpticalDiskDrive - True indicates that the machine has an optical disk drive (CD/DVD)\n- Census_TotalPhysicalRAM - Retrieves the physical RAM in MB\n- Census_ChassisTypeName - Retrieves a numeric representation of what type of chassis the machine has. A value of 0 means xx\n- Census_InternalPrimaryDiagonalDisplaySizeInInches - Retrieves the physical diagonal length in inches of the primary display\n- Census_InternalPrimaryDisplayResolutionHorizontal - Retrieves the number of pixels in the horizontal direction of the internal display\n- Census_InternalPrimaryDisplayResolutionVertical - Retrieves the number of pixels in the vertical direction of the internal display\n- Census_PowerPlatformRoleName - Indicates the OEM preferred power management profile. This value helps identify the basic form factor of the device\n- Census_InternalBatteryType - NA\n- Census_InternalBatteryNumberOfCharges - NA\n- Census_OSVersion - Numeric OS version Example - 10.0.10130.0\n- Census_OSArchitecture - Architecture on which the OS is based. Derived from OSVersionFull. Example - amd64\n- Census_OSBranch - Branch of the OS extracted from the OsVersionFull. Example - OsBranch = fbl_partner_eeap where OsVersion = 6.4.9813.0.amd64fre.fbl_partner_eeap.140810-0005\n- Census_OSBuildNumber - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or 10240\n- Census_OSBuildRevision - OS Build revision extracted from the OsVersionFull. Example - OsBuildRevision = 1000 or 16458\n- Census_OSEdition - Edition of the current OS. Sourced from HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion@EditionID in registry. Example: Enterprise\n- Census_OSSkuName - OS edition friendly name (currently Windows only)\n- Census_OSInstallTypeName - Friendly description of what install was used on the machine i.e. clean\n- Census_OSInstallLanguageIdentifier - NA\n- Census_OSUILocaleIdentifier - NA\n- Census_OSWUAutoUpdateOptionsName - Friendly name of the WindowsUpdate auto-update settings on the machine.\n- Census_IsPortableOperatingSystem - Indicates whether OS is booted up and running via Windows-To-Go on a USB stick.\n- Census_GenuineStateName - Friendly name of OSGenuineStateID. 0 = Genuine\n- Census_ActivationChannel - Retail license key or Volume license key for a machine.\n- Census_IsFlightingInternal - NA\n- Census_IsFlightsDisabled - Indicates if the machine is participating in flighting.\n- Census_FlightRing - The ring that the device user would like to receive flights for. This might be different from the ring of the OS which is currently installed if the user changes the ring after getting a flight from a different ring.\n- Census_ThresholdOptIn - NA\n- Census_FirmwareManufacturerIdentifier - NA\n- Census_FirmwareVersionIdentifier - NA\n- Census_IsSecureBootEnabled - Indicates if Secure Boot mode is enabled.\n- Census_IsWIMBootEnabled - NA\n- Census_IsVirtualDevice - Identifies a Virtual Machine (machine learning model)\n- Census_IsTouchEnabled - Is this a touch device?\n- Census_IsPenCapable - Is the device capable of pen input?\n- Census_IsAlwaysOnAlwaysConnectedCapable - Retrieves information about whether the battery enables the device to be AlwaysOnAlwaysConnected.\n- Wdft_IsGamer - Indicates whether the device is a gamer device or not based on its hardware combination.\n- Wdft_RegionIdentifier - NA", "answer": null, "metadata": {"task_id": "microsoft-malware-prediction", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/microsoft-malware-prediction/train.csv", "test_file": "./data/DSBench/data/data_resplit/microsoft-malware-prediction/test.csv", "answer_dir": "./data/DSBench/data/answers/microsoft-malware-prediction", "url": "https://www.kaggle.com/competitions/microsoft-malware-prediction"}}
{"prompt": "Description\nCan you extract meaning from a large, text-based dataset derived from inventions? Here's your chance to do so. \n\nThe U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its Open Data Portal. Patents are a form of intellectual property granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive vetting process prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity. \n\n\u201cThe USPTO serves an American innovation machine that never sleeps by granting patents, registering trademarks, and promoting intellectual property around the globe. The USPTO shares over 200 years' worth of human ingenuity with the world, from lightbulbs to quantum computers. Combined with creativity from the data science community, USPTO datasets carry unbounded potential to empower AI and ML models that will benefit the progress of science and society at large.\u201d\n\u2014 USPTO Chief Information Officer Jamie Holcombe\n\nIn this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims \"television set\" and a prior publication describes \"TV set\", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. \n\nThis extends beyond paraphrase identification; if one invention claims a \"strong material\" and another uses \"steel\", that may also be a match. What counts as a \"strong material\" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations. \n\nCan you build a model to match phrases in order to extract contextual information, thereby helping the patent community connect the dots between millions of patent documents?\n\nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation\nSubmissions are evaluated on the Pearson correlation coefficient between the predicted and actual similarity scores. \n\nSubmission File\nFor each id (representing a pair of phrases) in the test set, you must predict the similarity score. The file should contain a header and have the following format:\n```\nid,score\n4112d61851461f60,0\n09e418c93a776564,0.25\n36baf228038e314b,1\netc.\n```\n\nDataset Description\nIn this dataset, you are presented pairs of phrases (an anchor and a target phrase) and asked to rate how similar they are on a scale from 0 (not at all similar) to 1 (identical in meaning). This challenge differs from a standard semantic similarity task in that similarity has been scored here within a patent's context, specifically its CPC classification (version 2021.05), which indicates the subject to which the patent relates. For example, while the phrases \"bird\" and \"Cape Cod\" may have low semantic similarity in normal language, the likeness of their meaning is much closer if considered in the context of \"house\".\n\nThis is a code competition, in which you will submit code that will be run against an unseen test set. The unseen test set contains approximately 12k pairs of phrases. A small public test set has been provided for testing purposes but is not used in scoring. \n\nInformation on the meaning of CPC codes may be found on the USPTO website. The CPC version 2021.05 can be found on the CPC archive website.\n\nScore meanings\nThe scores are in the 0-1 range with increments of 0.25 with the following meanings:\n1.0 - Very close match. This is typically an exact match except possibly for differences in conjugation, quantity (e.g. singular vs. plural), and addition or removal of stopwords (e.g. \u201cthe\u201d, \u201cand\u201d, \u201cor\u201d).\n0.75 - Close synonym, e.g. \u201cmobile phone\u201d vs. \u201ccellphone\u201d. This also includes abbreviations, e.g. \"TCP\" -> \"transmission control protocol\".\n0.5 - Synonyms which don\u2019t have the same meaning (same function, same properties). This includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.\n0.25 - Somewhat related, e.g. the two phrases are in the same high level domain but are not synonyms. This also includes antonyms.\n0.0 - Unrelated.\n\nFiles\n- train.csv - the training set, containing phrases, contexts, and their similarity scores\n- test.csv - the test set, identical in structure to the training set but without the score\n- sample_submission.csv - a sample submission file in the correct format\n\nColumns\n- id - a unique identifier for a pair of phrases\n- anchor - the first phrase\n- target - the second phrase\n- context - the CPC classification (version 2021.05), which indicates the subject within which the similarity is to be scored\n- score - the similarity. This is sourced from a combination of one or more manual expert ratings.\n\n\"Google Patent Phrase Similarity Dataset\" by Google is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0)", "answer": null, "metadata": {"task_id": "us-patent-phrase-to-phrase-matching", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/us-patent-phrase-to-phrase-matching/train.csv", "test_file": "./data/DSBench/data/data_resplit/us-patent-phrase-to-phrase-matching/test.csv", "answer_dir": "./data/DSBench/data/answers/us-patent-phrase-to-phrase-matching", "url": "https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching"}}
{"prompt": "Description\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.\n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nGood luck and have fun!\n\nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\n\nwhere \\(\\hat{y}\\) is the predicted value, \\(y\\) is the ground truth value, and \\(n\\) is the number of rows in the test data.\n\nSubmission File\nFor each row id in the test set, you must predict the value of the target loss as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:\n\n```\nid,loss\n250000,0.0\n250001,10.3\n250002,42.42\netc.\n```\n\nDataset Description\nFor this competition, you will be predicting a target loss based on a number of feature columns given in the data. The ground truth loss is integer valued, although predictions can be continuous.\n\nFiles\n- `train.csv` - the training data with the target loss column\n- `test.csv` - the test set; you will be predicting the loss for each row in this file\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-aug-2021", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-aug-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-aug-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-aug-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-aug-2021"}}
{"prompt": "Description  \nThe August 2022 edition of the Tabular Playground Series is an opportunity to help the fictional company Keep It Dry improve its main product, Super Soaker. The product is used in factories to absorb spills and leaks. The company has just completed a large testing study for different product prototypes. Can you use this data to build a model that predicts product failures?\n\nAbout the Tabular Playground Series  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new to their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nGetting Started  \nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun! Photo above by freestocks on Unsplash\n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File  \nFor each id in the test set, you must predict a probability of failure. The file should contain a header and have the following format:  \n```\nid,failure  \n26570,0.2  \n26571,0.1  \n26572,0.9  \netc.\n```\n\nDataset Description  \nThis data represents the results of a large product testing study. For each product_code, you are given a number of product attributes (fixed for the code) as well as a number of measurement values for each individual product, representing various lab testing methods. Each product is used in a simulated real-world environment experiment and absorbs a certain amount of fluid (loading) to see whether or not it fails. Your task is to use the data to predict individual product failures of new codes with their individual lab test results.\n\nFiles  \n- train.csv - the training data, which includes the target `failure`\n- test.csv - the test set; your task is to predict the likelihood each `id` will experience a failure\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-aug-2022", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-aug-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-aug-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-aug-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-aug-2022"}}
{"prompt": "Evaluation\n\nSubmissions will be evaluated using Mean Absolute Error (MAE), where each \\(x_i\\) represents the predicted target, \\(y_i\\) represents the ground truth, and \\(n\\) is the number of rows in the test set.\n\nSubmission File\n\nFor each id in the test set, you must predict the target Age. The file should contain a header and have the following format:\n\n```\nid,yield\n74051,10.2\n74051,3.6\n74051,11.9\n```\netc.\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Crab Age Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nNote: You can use this notebook to generate additional synthetic data for this competition if you would like.\n\nFiles\n\n- `train.csv` - the training dataset; Age is the target\n- `test.csv` - the test dataset; your objective is to predict the probability of Age (the ground truth is int but you can predict int or float)\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e16", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e16/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e16/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e16", "url": "https://www.kaggle.com/competitions/playground-series-s3e16"}}
{"prompt": "Description\n\nWhat do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.\n\nCurrent simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.\n\nPartnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.\n\nIn this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.\n\nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.\n\nPhoto by Nino Liverani on Unsplash\n\nEvaluation\n\nThe competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:\n|\ud835\udc4b\u2212\ud835\udc4c| \nwhere \ud835\udc4b is the vector of predicted pressure and \ud835\udc4c is the vector of actual pressures across all breaths in the test set.\n\nSubmission File\n\nFor each id in the test set, you must predict a value for the pressure variable. The file should contain a header and have the following format:\n```\nid,pressure\n1,20\n2,23\n3,24\n```\netc.\n\nDataset Description\n\nThe ventilator data used in this competition was produced using a modified open-source ventilator connected to an artificial bellows test lung via a respiratory circuit. The diagram below illustrates the setup, with the two control inputs highlighted in green and the state variable (airway pressure) to predict in blue. \n\nThe first control input is a continuous variable from 0 to 100 representing the percentage the inspiratory solenoid valve is open to let air into the lung (i.e., 0 is completely closed and no air is let in and 100 is completely open). The second control input is a binary variable representing whether the exploratory valve is open (1) or closed (0) to let air out.\n\nIn this competition, participants are given numerous time series of breaths and will learn to predict the airway pressure in the respiratory circuit during the breath, given the time series of control inputs.\n\nEach time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below.\n\nFiles\n- train.csv: the training set\n- test.csv: the test set\n- sample_submission.csv: a sample submission file in the correct format\n\nColumns\n- id: globally-unique time step identifier across an entire file\n- breath_id: globally-unique time step for breaths\n- R: lung attribute indicating how restricted the airway is (in cmH2O/L/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can change R by changing the diameter of the straw, with higher R being harder to blow.\n- C: lung attribute indicating how compliant the lung is (in mL/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can change C by changing the thickness of the balloon\u2019s latex, with higher C having thinner latex and easier to blow.\n- time_step: the actual time stamp.\n- u_in: the control input for the inspiratory solenoid valve. Ranges from 0 to 100.\n- u_out: the control input for the exploratory solenoid valve. Either 0 or 1.\n- pressure: the airway pressure measured in the respiratory circuit, measured in cmH2O.", "answer": null, "metadata": {"task_id": "ventilator-pressure-prediction", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/ventilator-pressure-prediction/train.csv", "test_file": "./data/DSBench/data/data_resplit/ventilator-pressure-prediction/test.csv", "answer_dir": "./data/DSBench/data/answers/ventilator-pressure-prediction", "url": "https://www.kaggle.com/competitions/ventilator-pressure-prediction"}}
{"prompt": "Description  \nCan you find more cat in your dat?  \nWe loved the participation and engagement with the first Cat in the Dat competition.  \nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  \n- binary features  \n- low- and high-cardinality nominal features  \n- low- and high-cardinality ordinal features  \n- (potentially) cyclical features  \n\nThis follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables.  \nThis challenge adds the additional complexity of feature interactions, as well as missing data.  \n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.  \nIf you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.  \n\nHave Fun!\n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.  \n\nSubmission File  \nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:  \n```\nid,target  \n600000,0.5  \n600001,0.5  \n600002,0.5  \n...  \n```\n\nDataset Description  \nIn this competition, you will be predicting the probability [0, 1] of a binary target column.  \nThe data contains binary features (`bin_*`), nominal features (`nom_*`), ordinal features (`ord_*`) as well as (potentially cyclical) `day` (of the week) and `month` features. The string ordinal features `ord_{3-5}` are lexically ordered according to `string.ascii_letters`.  \n\nSince the purpose of this competition is to explore various encoding strategies, unlike the first Categorical Feature Encoding Challenge, the data for this challenge has missing values and feature interactions.  \n\nFiles  \n- `train.csv` - the training set  \n- `test.csv` - the test set; you must make predictions against this data  \n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "cat-in-the-dat-ii", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/cat-in-the-dat-ii/train.csv", "test_file": "./data/DSBench/data/data_resplit/cat-in-the-dat-ii/test.csv", "answer_dir": "./data/DSBench/data/answers/cat-in-the-dat-ii", "url": "https://www.kaggle.com/competitions/cat-in-the-dat-ii"}}
{"prompt": "Description  \nLong ago, in the distant, fragrant mists of time, there was a competition\u2026 It was not just any competition. It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples\u2026 without overfitting. Data scientists \u2015 including Kaggle's very own Will Cukierski \u2015 competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse. \n\nSo\u2026 we're doing it again.  \nDon't Overfit II: The Overfittening  \nThis is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend. In addition to bragging rights, the winner also gets swag. Enjoy!  \n\nAcknowledgments  \nWe hereby salute the hard work that went into the original competition, created by Phil Brierly. Thank you!\n\nEvaluation  \nSubmissions are evaluated using AUC ROC between the predicted target and the actual target value.  \n\nSubmission File  \nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:\n```\nid,target  \n300,0  \n301,0  \n302,0  \n303,0  \n304,0  \n305,0  \n306,0  \n307,0  \n308,0  \n```\n\nDataset Description  \nWhat am I predicting?  \nYou are predicting the binary target associated with each row, without overfitting to the minimal set of training examples provided.\n\nFiles  \n- train.csv - the training set. 250 rows.  \n- test.csv - the test set. 19,750 rows.  \n- sample_submission.csv - a sample submission file in the correct format\n\nColumns  \n- id - sample id  \n- target - a binary target of mysterious origin  \n- 0-299 - continuous variables", "answer": null, "metadata": {"task_id": "dont-overfit-ii", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/dont-overfit-ii/train.csv", "test_file": "./data/DSBench/data/data_resplit/dont-overfit-ii/test.csv", "answer_dir": "./data/DSBench/data/answers/dont-overfit-ii", "url": "https://www.kaggle.com/competitions/dont-overfit-ii"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 of the Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday at 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various models and feature engineering ideas, create visualizations, etc.\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nSubmissions will be evaluated based on MPA@3. Each submission can contain up to 3 predictions (all separated by spaces), and the earlier a correct prediction occurs, the higher score it will receive.\n\nFor each id in the test set, you must predict the target prognosis. The file should contain a header and have the following format:\n\nid, prognosis  \n707, Dengue West_Nile_fever Malaria  \n708, Lyme_disease West_Nile_fever Dengue  \n709, Dengue West_Nile_fever Lyme_disease  \netc.\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. Feature distributions are close to, but not exactly the same as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note that in the original dataset some prognoses contain spaces, but in the competition dataset spaces have been replaced with underscores to work with the MPA@K metric.\n\nFiles  \ntrain.csv - the training dataset; prognosis is the target  \ntest.csv - the test dataset; your objective is to predict prognosis  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e13", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e13/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e13/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e13", "url": "https://www.kaggle.com/competitions/playground-series-s3e13"}}
{"prompt": "Description  \nThis competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset. You are given 5 years of store-item sales data and asked to predict 3 months of sales for 50 different items at 10 different stores. \n\nWhat's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost? This is a great competition to explore different models and improve your skills in forecasting.\n\nEvaluation  \nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.\n\nKernel Submissions  \nYou can only make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.\n\nSubmission File  \nFor each id in the test set, you must predict a probability for the sales variable. The file should contain a header and have the following format:  \nid,sales  \n0,35  \n1,22  \n2,5  \netc.\n\nDataset Description  \nThe objective of this competition is to predict 3 months of item-level sales data at different store locations.\n\nFile descriptions  \ntrain.csv  \n- Training data  \ntest.csv  \n- Test data (Note: the Public/Private split is time based)  \nsample_submission.csv  \n- A sample submission file in the correct format  \n\nData fields  \ndate  \n- Date of the sale data. There are no holiday effects or store closures.  \nstore  \n- Store ID  \nitem  \n- Item ID  \nsales  \n- Number of items sold at a particular store on a particular date.", "answer": null, "metadata": {"task_id": "demand-forecasting-kernels-only", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/demand-forecasting-kernels-only/train.csv", "test_file": "./data/DSBench/data/data_resplit/demand-forecasting-kernels-only/test.csv", "answer_dir": "./data/DSBench/data/answers/demand-forecasting-kernels-only", "url": "https://www.kaggle.com/competitions/demand-forecasting-kernels-only"}}
{"prompt": "Description \n\nFor the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment `ATATGGCCTT` becomes `A 2 T 4 G 2 C 2`. Can you use this lossy information to accurately predict bacteria species?\n\nAbout the Tabular Playground Series\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.\n\nThe goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nGetting Started\n\nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nGood luck and have fun!\n\nAcknowledgements\n\nThe idea for this competition came from the following paper:\n\n@ARTICLE{10.3389/fmicb.2020.00257,\nAUTHOR ={Wood, Ryan L. and Jensen, Tanner and Wadsworth, Cindi and Clement, Mark and Nagpal, Prashant and Pitt, William G.},\nTITLE ={Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers},\nJOURNAL ={Frontiers in Microbiology},\nVOLUME ={11},\nYEAR ={2020},\nURL ={https://www.frontiersin.org/article/10.3389/fmicb.2020.00257},\nDOI ={10.3389/fmicb.2020.00257},\nISSN ={1664-302X}}\n\nEvaluation\n\nEvaluation\n\nSubmissions will be evaluated based on their categorization accuracy.\n\nSubmission Format\n\nThe submission format for the competition is a CSV file with the following format:\n```\nrow_id,target\n200000,Streptococcus_pneumoniae\n200001,Enterococcus_hirae\netc.\n```\n\nDataset Description\n\nFor this challenge, you will be predicting bacteria species based on repeated lossy measurements of DNA snippets. Snippets of length 10 are analyzed using Raman spectroscopy that calculates the histogram of bases in the snippet. In other words, the DNA segment `ATATGGCCTT` becomes `A 2 T 4 G 2 C 2`.\n\nEach row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g., `A0 T0 G0 C10` to `A10 T0 G0 C0`), which then has a bias spectrum (of totally random ATGC) subtracted from the results.\n\nThe data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging.\n\nFiles\n- `train.csv` - the training set, which contains the spectrum of 10-mer histograms for each sample\n- `test.csv` - the test set; your task is to predict the bacteria species (`target`) for each `row_id`\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-feb-2022", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-feb-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-feb-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-feb-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-feb-2022"}}
{"prompt": "Description  \nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!  \nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday at 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets  \nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation  \nRoot Mean Squared Log Error (RMSLE)  \nSubmissions are scored on the root mean squared log error (RMSLE) (the sklearn `mean_squared_log_error` with `squared=False`).\n\nSubmission File  \nFor each id in the test set, you must predict the value for the target cost. The file should contain a header and have the following format:\n\n```\nid,cost\n360336,99.615\n360337,87.203\n360338,101.111\n```\n\nDataset Description  \nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Media Campaign Cost Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- `train.csv` - the training dataset; `cost` is the target\n- `test.csv` - the test dataset; your objective is to predict `cost`\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e11", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e11/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e11/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e11", "url": "https://www.kaggle.com/competitions/playground-series-s3e11"}}
{"prompt": "**Description**\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. \n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. \n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. \n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features. \n\nGood luck and have fun!\n\nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\n**Evaluation**\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target. \n\n**Submission File**\n\nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:\n```\nid,target  \n600000,0.5  \n600001,0.9  \n600002,0.1  \netc.\n```\n\n**Dataset Description**\n\nFor this competition, you will be predicting a binary target based on 100 feature columns given in the data. All columns are continuous. The data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email. \n\n**Files**\n\n- `train.csv` - the training data with the target column\n- `test.csv` - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-nov-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-nov-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-nov-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-nov-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-nov-2021"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nWith the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\n### Synthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\n### Evaluation\n\n#### Root Mean Squared Error (RMSE)\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\\[ \\textrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} \\]\nwhere \\( \\hat{y}_i \\) is the predicted value and \\( y_i \\) is the original value for each instance \\( i \\).\n\n#### Submission File\n\nFor each id in the test set, you must predict the value for the target price. The file should contain a header and have the following format:\n```\nid,price\n193573,3969.155\n193574,8512.67\n193575,1122.34\netc.\n```\n\n### Dataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Gemstone Price Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\n#### Files\n\n- `train.csv` - the training dataset; `price` is the target\n- `test.csv` - the test dataset; your objective is to predict `price`\n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e8", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e8/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e8/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e8", "url": "https://www.kaggle.com/competitions/playground-series-s3e8"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly lightweight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nTo start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly lightweight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\nFor each EmployeeNumber in the test set, you must predict the probability for the target variable Attrition. The file should contain a header and have the following format:\n```\nEmployeeNumber, Attrition\n1677, 0.78\n1678, 0.34\n1679, 0.55\netc.\n```\n\nDataset Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on Employee Attrition. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- train.csv - the training dataset; Attrition is the binary target\n- test.csv - the test dataset; your objective is to predict the probability of positive Attrition\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e3", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e3/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e3/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e3", "url": "https://www.kaggle.com/competitions/playground-series-s3e3"}}
{"prompt": "Description\n\nThis is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity.\n\nBackground\n\nThe White House Office of Science and Technology Policy (OSTP) pulled together a coalition of research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from the National Academies of Sciences, Engineering, and Medicine (NASEM) and the World Health Organization (WHO).\n\nThe Challenge\n\nKaggle is launching a companion COVID-19 forecasting challenge to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19.\n\nYou are encouraged to pull in, curate, and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your findings in a notebook.\n\nAs the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).\n\nWe have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.\n\nCompanies and Organizations\n\nThere is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.\n\nAcknowledgements\n\nJHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.\n\nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation\n\nPublic and Private Leaderboard\n\nTo have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after the competition launch. Only use data prior to 2020-04-01 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.\n\nPublic Leaderboard Period - 2020-04-01 to 2020-04-15\nPrivate Leaderboard Period - 2020-04-16 to 2020-05-14\n\nEvaluation\n\nSubmissions are evaluated using the column-wise root mean squared logarithmic error. The RMSLE for a single column is calculated as:\n\\[\n\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2}\n\\]\n\nwhere:\n- \\(n\\) is the total number of observations\n- \\(p_i\\) is your prediction\n- \\(a_i\\) is the actual value \n- \\(\\log(x)\\) is the natural logarithm of \\(x\\)\n\nThe final score is the mean of the RMSLE over all columns (in this case, 2).\n\nSubmission File\n\nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFor each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:\n```\nForecastId,ConfirmedCases,Fatalities\n1,10,0\n2,10,0\n3,10,0\netc.\n```\nYou will get the ForecastId for the corresponding date and location from the test.csv file.\n\nDataset Description\n\nIn this challenge, you will be predicting the cumulative number of confirmed COVID-19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFiles\n- train.csv - the training data (you are encouraged to join in many more useful external datasets)\n- test.csv - the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on the last 28 days of predicted data.\n- submission.csv - a sample submission in the correct format; again, predictions should be cumulative.\n\nData Source\n\nThe evaluation data for this competition comes from John Hopkins CSSE, which is uninvolved in the competition. See their README for a description of how the data was collected. They are currently updating the data daily.", "answer": null, "metadata": {"task_id": "covid19-global-forecasting-week-4", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-4/train.csv", "test_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-4/test.csv", "answer_dir": "./data/DSBench/data/answers/covid19-global-forecasting-week-4", "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-4"}}
{"prompt": "Description\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict the probability of a Machine failure. The file should contain a header and have the following format:  \nid, Machine failure  \n136429, 0.5  \n136430, 0.1  \n136431, 0.9  \netc.\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Machine Failure Predictions. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- train.csv - The training dataset; Machine failure is the (binary) target (which, in order to align with the ordering of the original dataset, is not in the last column position)\n- test.csv - The test dataset; your objective is to predict the probability of Machine failure\n- sample_submission.csv - A sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e17", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e17/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e17/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e17", "url": "https://www.kaggle.com/competitions/playground-series-s3e17"}}
{"prompt": "Description\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset used for this competition is synthetic but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation\nSubmissions are evaluated on multi-class classification accuracy.\n\nSubmission File\nFor each Id in the test set, you must predict the Cover_Type class. The file should contain a header and have the following format: \nId,Cover_Type\n4000000,2 \n4000001,1 \n4000002,3 \netc.\n\nDataset Description\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data. Please refer to this data page for a detailed explanation of the features.\n\nFiles\ntrain.csv - the training data with the target Cover_Type column\ntest.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-dec-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-dec-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-dec-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-dec-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-dec-2021"}}
{"prompt": "Description  \nIs there a cat in your dat?  \nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.  \nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:  \n- binary features  \n- low- and high-cardinality nominal features  \n- low- and high-cardinality ordinal features  \n- (potentially) cyclical features  \n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.  \nIf you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.  \nHave Fun!\n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.  \n\nSubmission File  \nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:  \n```\nid,target  \n300000,0.5  \n300001,0.5  \n300002,0.5  \n...\n```\n\nDataset Description  \nIn this competition, you will be predicting the probability [0, 1] of a binary target column. The data contains binary features (`bin_*`), nominal features (`nom_*`), ordinal features (`ord_*`), as well as (potentially cyclical) day (of the week) and month features. The string ordinal features `ord_{3-5}` are lexically ordered according to `string.ascii_letters`.\n\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values. (Of course, in real-world settings both of these factors are often important to consider!)\n\nFiles  \n- `train.csv` - the training set  \n- `test.csv` - the test set; you must make predictions against this data  \n- `sample_submission.csv` - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "cat-in-the-dat", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/cat-in-the-dat/train.csv", "test_file": "./data/DSBench/data/data_resplit/cat-in-the-dat/test.csv", "answer_dir": "./data/DSBench/data/answers/cat-in-the-dat", "url": "https://www.kaggle.com/competitions/cat-in-the-dat"}}
{"prompt": "Description\n\nWe're going to make you an offer you can't refuse: a Kaggle competition!\n\nIn a world where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's \"You had me at 'Hello.'\" For others, the trailer falls short of expectations and you think \"What we have here is a failure to communicate.\"\n\nIn this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.\n\nJoin in, \"make our day,\" and then \"you've got to ask yourself one question: 'Do I feel lucky?'\"\n\nEvaluation\n\nIt is your job to predict the international box office revenue for each movie. For each id in the test set, you must predict the value of the revenue variable. Submissions are evaluated on Root-Mean-Squared-Logarithmic-Error (RMSLE) between the predicted value and the actual revenue. Logs are taken to not overweight blockbuster revenue movies.\n\nSubmission File Format\n\nThe file should contain a header and have the following format:\n\nid,revenue  \n1461,1000000  \n1462,50000  \n1463,800000000  \netc.  \n\nYou can download an example submission file (sample_submission.csv) on the Data page.\n\nDataset Description\n\nIn this dataset, you are provided with 7,398 movies and a variety of metadata obtained from The Movie Database (TMDB). Movies are labeled with id. Data points include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You are predicting the worldwide revenue for 4,398 movies in the test file.\n\nNote - many movies are remade over the years, therefore it may seem like multiple instances of a movie may appear in the data. However, they are different and should be considered separate movies. In addition, some movies may share a title but be entirely unrelated. E.g., The Karate Kid (id: 5266) was released in 1986, while a clearly (or maybe just subjectively) inferior remake (id: 1987) was released in 2010. Also, while the Frozen (id: 5295) released by Disney in 2013 may be the household name, don't forget about the less-popular Frozen (id: 139) released three years earlier about skiers who are stranded on a chairlift\u2026\n\nAcknowledgements\n\nThis dataset has been collected from TMDB. The movie details, credits, and keywords have been collected from the TMDB Open API. This competition uses the TMDB API but is not endorsed or certified by TMDB. Their API also provides access to data on many additional movies, actors, actresses, crew members, and TV shows. You can try it for yourself here.", "answer": null, "metadata": {"task_id": "tmdb-box-office-prediction", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tmdb-box-office-prediction/train.csv", "test_file": "./data/DSBench/data/data_resplit/tmdb-box-office-prediction/test.csv", "answer_dir": "./data/DSBench/data/answers/tmdb-box-office-prediction", "url": "https://www.kaggle.com/competitions/tmdb-box-office-prediction"}}
{"prompt": "Description\n\nComputers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences. Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context\u2014something computers aren't trained to do well\u2026 yet. Questions can take many forms\u2014some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simply right or wrong.\n\nUnfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\n\nIn this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this dataset. What you see is what you get! Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.\n\nEvaluation\n\nSubmissions are evaluated on the mean column-wise Spearman's correlation coefficient. The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.\n\nSubmission File\n\nFor each `qa_id` in the test set, you must predict a probability for each target variable. The predictions should be in the range [0, 1]. The file should contain a header and have the following format:\n```\nqa_id, question_asker_intent_understanding, ..., answer_well_written\n6, 0.0, ..., 0.5\n8, 0.5, ..., 0.1\n18, 1.0, ..., 0.0\netc.\n```\n\nDataset Description\n\nThe data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair. The list of 30 target labels are the same as the column names in the `sample_submission.csv` file. Target labels with the prefix `question_` relate to the `question_title` and/or `question_body` features in the data. Target labels with the prefix `answer_` relate to the `answer` feature. Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions. This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0, 1]. Therefore, predictions must also be in that range. Since this is a synchronous re-run competition, you only have access to the Public test set. For planning purposes, the re-run test set is no larger than 10,000 rows and less than 8 Mb uncompressed. Additional information about the labels and collection method will be provided by the competition sponsor in the forum.\n\nFile descriptions\n\n`train.csv` - the training data (target labels are the last 30 columns)\n\n`test.csv` - the test set (you must predict 30 labels for each test set row)\n\n`sample_submission.csv` - a sample submission file in the correct format; column names are the 30 target labels", "answer": null, "metadata": {"task_id": "google-quest-challenge", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/google-quest-challenge/train.csv", "test_file": "./data/DSBench/data/data_resplit/google-quest-challenge/test.csv", "answer_dir": "./data/DSBench/data/answers/google-quest-challenge", "url": "https://www.kaggle.com/competitions/google-quest-challenge"}}
{"prompt": "Evaluation  \nSubmissions are evaluated using the R2 score.\n\nSubmission File  \nFor each id row in the test set, you must predict the value of the target, FloodProbability. The file should contain a header and have the following format: \n```\nid,FloodProbability  \n1117957,0.5  \n1117958,0.5  \n1117959,0.5  \netc.\n```\n\nDataset Description  \nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Flood Prediction Factors dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.  \nNote: This dataset is particularly well suited for visualizations, clustering, and general EDA. Show off your skills!\n\nFiles  \n- train.csv - the training dataset; FloodProbability is the target  \n- test.csv - the test dataset; your objective is to predict the FloodProbability for each row  \n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s4e5", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s4e5/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s4e5/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s4e5", "url": "https://www.kaggle.com/competitions/playground-series-s4e5"}}
{"prompt": "**Description**\n\nEssay writing is an important method to evaluate student learning and performance. It is also time-consuming for educators to grade by hand. Automated Writing Evaluation (AWE) systems can score essays to supplement an educator\u2019s other efforts. AWEs also allow students to receive regular and timely feedback on their writing. However, due to their costs, many advancements in the field are not widely available to students and educators. Open-source solutions to assess student writing are needed to reach every community with these important educational tools.\n\nPrevious efforts to develop open-source AWEs have been limited by small datasets that were not nationally diverse or focused on common essay formats. The first Automated Essay Scoring competition scored student-written short-answer responses; however, this is a writing task not often used in the classroom. To improve upon earlier efforts, a more expansive dataset that includes high-quality, realistic classroom writing samples was required. Further, to broaden the impact, the dataset should include samples across economic and location populations to mitigate the potential of algorithmic bias.\n\nIn this competition, you will work with the largest open-access writing dataset aligned to current standards for student-appropriate assessments. Can you help produce an open-source essay scoring algorithm that improves upon the original Automated Student Assessment Prize (ASAP) competition hosted in 2012?\n\nCompetition host Vanderbilt University is a private research university in Nashville, Tennessee. For this competition, Vanderbilt has partnered with The Learning Agency Lab, an Arizona-based independent nonprofit focused on developing the science of learning-based tools and programs for the social good.\n\nTo ensure the results of this competition are widely available, winning solutions will be released as open source. More robust and accessible AWE options will help more students get the frequent feedback they need and provide educators with additional support, especially in underserved districts.\n\n**Acknowledgments**\n\nVanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and the Chan Zuckerberg Initiative for their support in making this work possible.\n\n**Evaluation**\n\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe quadratic weighted kappa is calculated as follows:\n\nFirst, an N x N histogram matrix O is constructed, such that O_i,j corresponds to the number of essay_ids i (actual) that received a predicted value j.\n\nAn N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:\n\n\\[ w_{i,j} = \\frac{(i - j)^2 }{(N - 1)^2} \\]\n\nAn N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values. This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated as:\n\n\\[ \\kappa = 1 - \\frac{\\sum_{i,j} w_{i,j} O_{i,j}}{\\sum_{i,j} w_{i,j} E_{i,j}} \\]\n\n**Submission File**\n\nFor each essay_id in the test set, you must predict the corresponding score (described on the Data page). The file should contain a header and have the following format:\n\n```\nessay_id,score\n000d118,3\n000fe60,3\n001ab80,4\n...\n```\n\n**Dataset Description**\n\nThe competition dataset comprises about 24,000 student-written argumentative essays. Each essay was scored on a scale of 1 to 6 ([Link to the Holistic Scoring Rubric](#)). Your goal is to predict the score an essay received from its text.\n\n**File and Field Information**\n\n- **train.csv**\n  - Essays and scores to be used as training data.\n  - **essay_id** - The unique ID of the essay\n  - **full_text** - The full essay response\n  - **score** - Holistic score of the essay on a 1-6 scale\n\n- **test.csv**\n  - The essays to be used as test data. Contains the same fields as train.csv, aside from exclusion of the score. (Note: The rerun test set has approximately 8k observations.)\n\n- **sample_submission.csv**\n  - A submission file in the correct format.\n  - **essay_id** - The unique ID of the essay\n  - **score** - The predicted holistic score of the essay on a 1-6 scale\n\nPlease note that this is a **Code Competition**.", "answer": null, "metadata": {"task_id": "learning-agency-lab-automated-essay-scoring-2", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/learning-agency-lab-automated-essay-scoring-2/train.csv", "test_file": "./data/DSBench/data/data_resplit/learning-agency-lab-automated-essay-scoring-2/test.csv", "answer_dir": "./data/DSBench/data/answers/learning-agency-lab-automated-essay-scoring-2", "url": "https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2"}}
{"prompt": "Welcome to one of our \"Getting Started\" competitions \ud83d\udc4b This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don\u2019t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies). But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. Take this example:\n\nThe author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine.\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which ones aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n\ud83d\udca1Getting Started Notebook\n\nTo get started quickly, feel free to take advantage of this starter notebook.\n\nAcknowledgments\n\nThis dataset was created by the company figure-eight and originally shared on their \u2018Data For Everyone\u2019 website here. Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480\n\nSubmissions are evaluated using F1 between the predicted and expected answers. F1 is calculated as follows:\n\n\\[ F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\]\n\nwhere:\n\n\\[\n\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\\[\n\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\nand:\n\nTrue Positive (TP) = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n\nFalse Positive (FP) = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n\nFalse Negative (FN) = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.\n\nFor each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:\n\n```\nid,target\n0,0\n2,0\n3,1\n9,0\n11,0\n```\n\nWhat files do I need?\nYou'll need train.csv, test.csv, and sample_submission.csv.\n\nWhat should I expect the data format to be?\nEach sample in the train and test set has the following information:\n- The text of a tweet\n- A keyword from that tweet (although this may be blank!)\n- The location the tweet was sent from (may also be blank)\n\nWhat am I predicting?\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nFiles:\n- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format\n\nColumns:\n- id - a unique identifier for each tweet\n- text - the text of the tweet\n- location - the location the tweet was sent from (may be blank)\n- keyword - a particular keyword from the tweet (may be blank)\n- target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)", "answer": null, "metadata": {"task_id": "nlp-getting-started", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/nlp-getting-started/train.csv", "test_file": "./data/DSBench/data/data_resplit/nlp-getting-started/test.csv", "answer_dir": "./data/DSBench/data/answers/nlp-getting-started", "url": "https://www.kaggle.com/competitions/nlp-getting-started"}}
{"prompt": "Description  \nCan machine learning identify the appropriate reading level of a passage of text and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\n\nCurrently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number of words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nIf successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n\nAcknowledgements  \nCommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project. The organizers would like to thank Schmidt Futures for their advice and support for making this work possible.\n\nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation  \nSubmissions are scored on the root mean squared error. RMSE is defined as: \n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\nwhere \\(\\hat{y}\\) is the predicted value, \\(y\\) is the original value, and \\(n\\) is the number of rows in the test data.\n\nSubmission File  \nFor each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format: \n```\nid,target \neaf8e7355,0.0 \n60ecc9777,0.5 \nc0f722661,-2.0 \netc.\n```\n\nDataset Description  \nIn this competition, we're predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores. Note that the test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set.\n\nAlso note that while licensing information is provided for the public test set (because the associated excerpts are available for display/use), the hidden private test set includes only blank license/legal information.\n\nFiles  \n- train.csv - the training set\n- test.csv - the test set\n- sample_submission.csv - a sample submission file in the correct format\n\nColumns  \n- id - unique ID for excerpt\n- url_legal - URL of source - this is blank in the test set.\n- license - license of source material - this is blank in the test set.\n- excerpt - text to predict reading ease of\n- target - reading ease\n- standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\nUpdate  \nThis dataset, the CLEAR Corpus, has now been released in full. You may obtain it from either of the following locations:\n- commonlit.org\n- github.com\n\nThe full corpus contains an expanded set of fields as well as six readability predictions on each excerpt resulting from this competition.\n\nYou may read more about the CLEAR Corpus from the following publications:\n- Crossley, S. A., Heintz, A., Choi, J., Batchelor, J., Karimi, M., & Malatinszky, A. (in press). A large-scaled corpus for assessing text readability. Behavior Research Methods. [link]\n- Crossley, S. A., Heintz, A., Choi, J., Batchelor, J., & Karimi, M. (2021). The CommonLit Ease of Readability (CLEAR) Corpus. Proceedings of the 14th International Conference on Educational Data Mining (EDM). Paris, France. [link]", "answer": null, "metadata": {"task_id": "commonlitreadabilityprize", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/commonlitreadabilityprize/train.csv", "test_file": "./data/DSBench/data/data_resplit/commonlitreadabilityprize/test.csv", "answer_dir": "./data/DSBench/data/answers/commonlitreadabilityprize", "url": "https://www.kaggle.com/competitions/commonlitreadabilityprize"}}
{"prompt": "Description  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation. The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features. Good luck and have fun! For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File  \nFor each id in the test set, you must predict a probability for the claim variable. The file should contain a header and have the following format:  \nid, claim  \n957919, 0.5  \n957920, 0.5  \n957921, 0.5  \netc.\n\nDataset Description  \nFor this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nFiles  \ntrain.csv - the training data with the target claim column  \ntest.csv - the test set; you will be predicting the claim for each row in this file  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-sep-2021", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-sep-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-sep-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-sep-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-sep-2021"}}
{"prompt": "Description  \nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation  \nSubmissions are evaluated on micro-averaged F1-Score between predicted and actual values.\n\nSubmission File  \nFor each id in the test set, you must predict the corresponding outcome. The file should contain a header and have the following format:  \n\nid,outcome  \n1235,lived  \n1236,lived  \n1237,died  \netc.\n\nDataset Description  \nThe dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Horse Survival Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Good luck!\n\nFiles  \ntrain.csv - the training dataset; outcome is the (categorical) target  \ntest.csv - the test dataset; your objective is to predict outcome  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e22", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e22/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e22/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e22", "url": "https://www.kaggle.com/competitions/playground-series-s3e22"}}
{"prompt": "Description  \nThis week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version and make a submission there. This is week 3 of Kaggle's COVID-19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity.\n\nBackground  \nThe White House Office of Science and Technology Policy (OSTP) pulled together a coalition of research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from the National Academies of Sciences, Engineering, and Medicine (NASEM) and the World Health Organization (WHO).\n\nThe Challenge  \nKaggle is launching companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It\u2019s also to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate, and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your findings in a notebook. As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.\n\nCompanies and Organizations  \nThere is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle\u2019s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.\n\nAcknowledgements  \nJHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control. This is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation  \nPublic and Private Leaderboard  \nTo have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after competition launch. Only use data prior to 2020-03-26 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period. Public Leaderboard Period - 2020-03-26 - 2020-04-08 Private Leaderboard Period - 2020-04-09 - 2020-05-07 \n\nEvaluation  \nSubmissions are evaluated using the column-wise root mean squared logarithmic error (RMSLE). The RMSLE for a single column is calculated as:\n\n\\[ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2 } \\]\n\nwhere:  \n\\( n \\) is the total number of observations  \n\\( p_i \\) is your prediction  \n\\( a_i \\) is the actual value  \n\\( \\log(x) \\) is the natural logarithm of \\( x \\)\n\nThe final score is the mean of the RMSLE over all columns (in this case, 2).\n\nSubmission File  \nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFor each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:  \n`ForecastId,ConfirmedCases,Fatalities`  \n`1,10,0`  \n`2,10,0`  \n`3,10,0`  \netc.  \n\nYou will get the ForecastId for the corresponding date and location from the test.csv file.\n\nDataset Description  \nIn this challenge, you will be predicting the cumulative number of confirmed COVID-19 cases in various locations across the world, as well as the number of resulting fatalities, for future dates. We understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFiles  \n- train.csv: the training data (you are encouraged to join in many more useful external datasets)\n- test.csv: the dates to predict; there is a week of overlap with the training data for the initial Public leaderboard. Once submissions are paused, the Public leaderboard will update based on the last 28 days of predicted data.\n- submission.csv: a sample submission in the correct format; again, predictions should be cumulative.\n\nData Source  \nThis evaluation data for this competition comes from Johns Hopkins CSSE, which is uninvolved in the competition. See their README for a description of how the data was collected. They are currently updating the data daily.", "answer": null, "metadata": {"task_id": "covid19-global-forecasting-week-3", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-3/train.csv", "test_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-3/test.csv", "answer_dir": "./data/DSBench/data/answers/covid19-global-forecasting-week-3", "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-3"}}
{"prompt": "Description  \nThink you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.  \nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real-world noise to emulate what scientists observe in laboratory experiments.\n\nTechnology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.  \nAcknowledgements:  \nThis would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).\n\nEvaluation  \nSubmissions are evaluated using the macro F1 score.  \nF1 is calculated as follows:  \n\\[ F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\]\nwhere:  \n\\[ \\text{precision} = \\frac{TP}{TP + FP} \\]  \n\\[ \\text{recall} = \\frac{TP}{TP + FN} \\]  \nIn \"macro\" F1, a separate F1 score is calculated for each open_channels value and then averaged.  \n\nSubmission File  \nFor each time value in the test set, you must predict open_channels. The files must have a header and should look like the following:\n```\ntime, open_channels \n500.0000, 0 \n500.0001, 2 \n...\n```\n\nDataset Description  \nIn this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.  \nIMPORTANT:  \nWhile the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001. You can find detailed information about the data from the paper \"Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data.\"\n\nFiles  \ntrain.csv - the training set  \ntest.csv - the test set; you will be predicting open_channels from the signal data in this file  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "liverpool-ion-switching", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/liverpool-ion-switching/train.csv", "test_file": "./data/DSBench/data/data_resplit/liverpool-ion-switching/test.csv", "answer_dir": "./data/DSBench/data/answers/liverpool-ion-switching", "url": "https://www.kaggle.com/competitions/liverpool-ion-switching"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!\n\nWith the same goal to give the Kaggle community a variety of fairly lightweight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday at 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly lightweight datasets that are synthetically generated from real-world data and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nThis episode is similar to the Kaggle/Zindi Hackathon that was held at the Kaggle@ICLR 2023: ML Solutions in Africa workshop in Rwanda, and builds on an ongoing partnership between Kaggle and Zindi to build community-driven impact across Africa. Zindi is a professional network for data scientists to learn, grow their careers, and get jobs. If you haven't done so recently, stop by Zindi and see what they're up to!\n\nPredicting CO2 Emissions\nThe ability to accurately monitor carbon emissions is a critical step in the fight against climate change. Precise carbon readings allow researchers and governments to understand the sources and patterns of carbon mass output. While Europe and North America have extensive systems in place to monitor carbon emissions on the ground, there are few available in Africa.\n\nThe objective of this challenge is to create machine learning models using open-source CO2 emissions data from Sentinel-5P satellite observations to predict future carbon emissions. These solutions may help enable governments and other actors to estimate carbon emission levels across Africa, even in places where on-the-ground monitoring is not possible.\n\nAcknowledgements\nWe acknowledge Carbon Monitor for the use of the GRACED dataset, and special thanks to Darius Moruri from Zindi for his work in preparing the dataset and starter notebooks.\n\nEvaluation\nRoot Mean Squared Error (RMSE)\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n\\[ \\textrm{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 } \\]\n\nwhere \\( \\hat{y}_i \\) is the predicted value and \\( y_i \\) is the original value for each instance \\( i \\).\n\nSubmission File\nFor each ID_LAT_LON_YEAR_WEEK row in the test set, you must predict the value for the target emission. The file should contain a header and have the following format:\n\n```plaintext\nID_LAT_LON_YEAR_WEEK,emission\nID_-0.510_29.290_2022_00,81.94\nID_-0.510_29.290_2022_01,81.94\nID_-0.510_29.290_2022_02,81.94\netc.\n```\n\nDataset Description\nThe objective of this challenge is to create machine learning models that use open-source emissions data (from Sentinel-5P satellite observations) to predict carbon emissions. Approximately 497 unique locations were selected from multiple areas in Rwanda, with a distribution around farmlands, cities, and power plants. The data for this competition is split by time; the years 2019-2021 are included in the training data, and your task is to predict the CO2 emissions data for 2022 through November.\n\nSeven main features were extracted weekly from Sentinel-5P from January 2019 to November 2022. Each feature (Sulphur Dioxide, Carbon Monoxide, etc.) contains sub-features such as column_number_density, which is the vertical column density at ground level, calculated using the DOAS technique. You can read more about each feature in the links below, including how they are measured and variable definitions. You are given the values of these features in the test set and your goal is to predict CO2 emissions using time information as well as these features.\n\n- Sulphur Dioxide - COPERNICUS/S5P/NRTI/L3_SO2\n- Carbon Monoxide - COPERNICUS/S5P/NRTI/L3_CO\n- Nitrogen Dioxide - COPERNICUS/S5P/NRTI/L3_NO2\n- Formaldehyde - COPERNICUS/S5P/NRTI/L3_HCHO\n- UV Aerosol Index - COPERNICUS/S5P/NRTI/L3_AER_AI\n- Ozone - COPERNICUS/S5P/NRTI/L3_O3\n- Cloud - COPERNICUS/S5P/OFFL/L3_CLOUD\n\nImportant: Please only use the data provided for this challenge as part of your modeling effort. Do not use any external data, including any data from Sentinel-5P not provided on this page.\n\nFiles\n- train.csv - the training set\n- test.csv - the test set; your task is to predict the emission target for each week at each location\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e20", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e20/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e20/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e20", "url": "https://www.kaggle.com/competitions/playground-series-s3e20"}}
{"prompt": "### Description\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!\n\nWith the same goal to give the Kaggle community a variety of fairly lightweight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly lightweight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\n### Synthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\n### Evaluation\n\nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.\n\n### Submission File\n\nFor each id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:\n\n```\nid, num_sold\n136950, 100\n136951, 100\n136952, 100\netc.\n```\n\n### Dataset Description\n\nFor this challenge, you will be predicting a full year worth of sales for various fictitious learning modules from different fictitious Kaggle-branded stores in different (real!) countries. This dataset is completely synthetic, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the task of predicting sales during the year 2022.\n\nGood luck!\n\n### Files\n\n#### train.csv\n- The training set, which includes the sales data for each date-country-store-item combination.\n\n#### test.csv\n- The test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n\n#### sample_submission.csv\n- A sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "playground-series-s3e19", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e19/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e19/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e19", "url": "https://www.kaggle.com/competitions/playground-series-s3e19"}}
{"prompt": "Description\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly lightweight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nTo start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly lightweight datasets that are synthetically generated from real-world data and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict the value for the target Class. The file should contain a header and have the following format:\n```\nid,Class\n341588,0.23\n341589,0.92\n341590,0.02\netc.\n```\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nNote, this base dataset for this competition was much larger than previous Tabular Tuesdays datasets, and thus may contain more artifacts than the last three competitions.\n\nFiles\n- train.csv - the training dataset; Class is the target\n- test.csv - the test dataset; your objective is to predict Class\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e4", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e4/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e4/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e4", "url": "https://www.kaggle.com/competitions/playground-series-s3e4"}}
{"prompt": "Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.\n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\nGood luck and have fun!\n\nCheck out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\n**Evaluation**\n\nSubmissions are scored on the root mean squared error (RMSE). RMSE is defined as:  \n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\n\nwhere \\(\\hat{y}\\) is the predicted value, \\(y\\) is the original value, and \\(n\\) is the number of rows in the test data.\n\n**Submission File**\n\nFor each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:  \n```\nid,target\n0,0.5\n5,10.2\n15,2.2\netc.\n```\n\n**Dataset Description**\n\nFor this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9, are categorical, and the feature columns, cont0 - cont13, are continuous.\n\n**Files**\n\n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-feb-2021", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-feb-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-feb-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-feb-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-feb-2021"}}
{"prompt": "Description  \nThe competing Kaggle merchandise stores we saw in January's Tabular Playground are at it again. This time, they're selling books! The task for this month's competitions is a bit more complicated. Not only are there six countries and four books to forecast, but you're being asked to forecast sales during the tumultuous year 2021. Can you use your data science skills to predict book sales when conditions are far from the ordinary?  \n\nAbout the Tabular Playground Series  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly. The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard. For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.  \n\nGetting Started  \nFor ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn. Good luck and have fun!  \nPhoto above by Aron Visuals on Unsplash\n\nEvaluation  \nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.  \n\nSubmission File  \nFor each row_id in the test set, you must predict the corresponding num_sold. The file should contain a header and have the following format:  \nrow_id,num_sold  \n70128,100  \n70129,100  \n70130,100 etc.  \n\nDataset Description  \nFor this challenge, you will be predicting a full year worth of sales for 4 items from two competing stores located in six different countries. This dataset is completely fictional but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the challenging task of predicting book sales during the year 2021. Good luck!\n\nFiles  \ntrain.csv - the training set, which includes the sales data for each date-country-store-item combination.  \ntest.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.  \nsample_submission.csv - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "tabular-playground-series-sep-2022", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-sep-2022/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-sep-2022/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-sep-2022", "url": "https://www.kaggle.com/competitions/tabular-playground-series-sep-2022"}}
{"prompt": "Description\n\nThis week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version and make a submission there. This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for California, USA.\n\nBackground\n\nThe White House Office of Science and Technology Policy (OSTP) pulled together a coalition of research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from the National Academies of Sciences, Engineering, and Medicine (NASEM) and the World Health Organization (WHO).\n\nThe Challenge\n\nKaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal isn't to produce accurate forecasts. It\u2019s to identify factors that appear to impact the transmission rate of COVID-19. You are encouraged to pull in, curate, and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your findings in a notebook. \n\nAs the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.\n\nCompanies and Organizations\n\nThere is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle's dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.\n\nAcknowledgements\n\nJHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.\n\nThis is a Code Competition. Refer to Code Requirements for details.\n\nEvaluation\n\nPublic and Private Leaderboards\n\nTo have a public leaderboard for this forecasting task, we will be using data from 7 days before to 7 days after the competition launch. Only use data on or prior to 2020-03-11 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.\n\nPublic Leaderboard Period: 2020-03-12 - 2020-03-25\nPrivate Leaderboard Period: 2020-03-26 - 2020-04-23\n\nEvaluation\n\nSubmissions are evaluated using the column-wise root mean squared logarithmic error (RMSLE). The RMSLE for a single column is calculated as:\n\n\\[ \\text{RMSLE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i + 1))^2 } \\]\n\nwhere:\n- \\( n \\) is the total number of observations\n- \\( p_i \\) is your prediction\n- \\( a_i \\) is the actual value\n- \\( \\log(x) \\) is the natural logarithm of \\( x \\)\n\nThe final score is the mean of the RMSLE over all columns (in this case, 2).\n\nSubmission File\n\nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFor each ForecastId in the test set, you'll predict the cumulative COVID-19 cases and fatalities to date. The file should contain a header and have the following format:\n\n```\nForecastId,ConfirmedCases,Fatalities\n1,10,0\n2,10,0\n3,10,0\n```\n\nYou will get the ForecastId for the corresponding date and location from the test.csv file.\n\nDataset Description\n\nIn this challenge, you will be predicting the cumulative number of confirmed COVID-19 cases in various locations across the world, as well as the number of resulting fatalities for future dates. \n\nWe understand this is a serious situation, and in no way want to trivialize the human impact this crisis is causing by predicting fatalities. Our goal is to provide better methods for estimates that can assist medical and governmental institutions to prepare and adjust as pandemics unfold.\n\nFiles\n\n- train.csv - the training data up to Mar 18, 2020.\n- test.csv - the dates to predict; there is a week of overlap with the training data for the initial public leaderboard. Once submissions are paused, the public leaderboard will update based on the last 28 days of predicted data.\n- submission.csv - a sample submission in the correct format; again, predictions should be cumulative.\n\nData Source\n\nThe evaluation data for this competition comes from Johns Hopkins CSSE, which is uninvolved in the competition. See their README for a description of how the data was collected. They are currently updating the data daily.", "answer": null, "metadata": {"task_id": "covid19-global-forecasting-week-1", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-1/train.csv", "test_file": "./data/DSBench/data/data_resplit/covid19-global-forecasting-week-1/test.csv", "answer_dir": "./data/DSBench/data/answers/covid19-global-forecasting-week-1", "url": "https://www.kaggle.com/competitions/covid19-global-forecasting-week-1"}}
{"prompt": "Description\n\nWelcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes.\n\nFirst, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images.\n\nSecond, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!); we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion.\n\nRegardless of these changes, the goals of the Playground Series remain the same\u2014to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective!\n\nWith the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets\n\nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation\n\nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict the value for the target booking_status. The file should contain a header and have the following format:\n\n```\nid,booking_status\n42100,0\n42101,1\n42102,0\n```\netc.\n\nDataset Description\n\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Reservation Cancellation Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\n- train.csv - the training dataset; booking_status is the target (e.g., whether the reservation was cancelled).\n- test.csv - the test dataset; your objective is to predict booking_status.\n- sample_submission.csv - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "playground-series-s3e7", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e7/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e7/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e7", "url": "https://www.kaggle.com/competitions/playground-series-s3e7"}}
{"prompt": "Description\n\n\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second, it is hard to tell whether the sentiment behind a specific tweet will impact a company\u2019s or a person's brand by being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But which words actually lead to the sentiment description? In this competition, you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nHelp build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n\nIn this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under the Creative Commons Attribution 4.0 International Licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\nEvaluation\n\nThe metric in this competition is the [word-level Jaccard score](https://en.wikipedia.org/wiki/Jaccard_index). A good description of Jaccard similarity for strings is [here](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50).\n\nA Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below:\n\n```python\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n```\n\nThe formula for the overall metric, then, is: \n\\[ \\text{score} = \\frac{1}{n} \\sum_{i=1}^n \\text{jaccard}( \\text{gt}_i, \\text{dt}_i ) \\]\nwhere: \n- \\( n \\) = number of documents\n- \\( \\text{jaccard} \\) = the function provided above\n- \\( \\text{gt}_i \\) = the ith ground truth\n- \\( \\text{dt}_i \\) = the ith prediction\n\n## Submission File\n\nFor each ID in the test set, you must predict the string that best supports the sentiment for the tweet in question. Note that the selected text _needs_ to be **quoted** and **complete** (include punctuation, etc. - the above code splits ONLY on whitespace) to work correctly. The file should contain a header and have the following format:\n\n```plaintext\ntextID,selected_text\n2,\"very good\"\n5,\"I don't care\"\n6,\"bad\"\n8,\"it was, yes\"\netc.\n```\n\nDataset Description\n\nWhat files do I need?\nYou'll need `train.csv`, `test.csv`, and `sample_submission.csv`.\n\nWhat should I expect the data format to be?\nEach row contains the `text` of a tweet and a `sentiment` label. In the training set, you are provided with a word or phrase drawn from the tweet (`selected_text`) that encapsulates the provided sentiment. Make sure, when parsing the CSV, to remove the beginning/ending quotes from the `text` field to ensure that you don't include them in your training.\n\nWhat am I predicting?\nYou're attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.). The format is as follows:\n\n```plaintext\n<id>,\"<word or phrase that supports the sentiment>\"\nFor example:\n2,\"very good\"\n5,\"I am neutral about this\"\n6,\"bad\"\n8,\"if you say so!\"\netc.\n```\n\nFiles\n- `train.csv` - the training set\n- `test.csv` - the test set\n- `sample_submission.csv` - a sample submission file in the correct format\n\nColumns\n- `textID` - unique ID for each piece of text\n- `text` - the text of the tweet\n- `sentiment` - the general sentiment of the tweet\n- `selected_text` - [train only] the text that supports the tweet's sentiment.", "answer": null, "metadata": {"task_id": "tweet-sentiment-extraction", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tweet-sentiment-extraction/train.csv", "test_file": "./data/DSBench/data/data_resplit/tweet-sentiment-extraction/test.csv", "answer_dir": "./data/DSBench/data/answers/tweet-sentiment-extraction", "url": "https://www.kaggle.com/competitions/tweet-sentiment-extraction"}}
{"prompt": "Description  \nFrom frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.  \nSantander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.  \nIn this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.  \n\nEvaluation  \nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.  \n\nSubmission File  \nFor each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:  \nID,TARGET  \n2,0  \n5,0  \n6,0  \netc.  \n\nDataset Description  \nYou are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers.  \nThe task is to predict the probability that each customer in the test set is an unsatisfied customer.  \n\nFile descriptions  \ntrain.csv - the training set including the target  \ntest.csv - the test set without the target  \nsample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "santander-customer-satisfaction", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/santander-customer-satisfaction/train.csv", "test_file": "./data/DSBench/data/data_resplit/santander-customer-satisfaction/test.csv", "answer_dir": "./data/DSBench/data/answers/santander-customer-satisfaction", "url": "https://www.kaggle.com/competitions/santander-customer-satisfaction"}}
{"prompt": "Description\nWelcome to Instant (well, almost) Gratification!\n\nIn 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy\u2014essentially a blinking cursor in a docker container\u2014into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface.\n\nWe have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition. However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work.\n\nSay Hello to Synchronous KO\n\nWe're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle:\n\n- With a truly withheld test set, we are practicing proper, rigorous machine learning.\n- We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions.\n- You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails.\n- We will run all submissions against the private data, not just selected ones.\n- Participants will get the complete and familiar public/private scores available in a traditional competition.\n- The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels.\n\nThis competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes.\n\nSince the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!\n\nEvaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nSubmission File\n\nFor each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format:\n\nid,target\nba88c155ba898fc8b5099893036ef205,0.5\n7cbab5cea99169139e7e6d8ff74ebb77,0.5\n7baaf361537fbd8a1aaa2c97a6d4ccc7,0.5\netc.\n\nDataset Description\n\nThis is an anonymized, binary classification dataset found on a USB stick that washed ashore in a bottle. There was no data dictionary with the dataset, but this poem was handwritten on an accompanying scrap of paper:\n\nSilly column names abound,\nbut the test set is a mystery.\nCareful how you pick and slice,\nor be left behind by history.\n\nFiles\n\nIn a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different ids, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it predicts on the public test.csv in the format specified by the public sample_submission.csv, but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.\n\n- train.csv - the training set\n- test.csv - the test set (you must predict the target value for these variables)\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "instant-gratification", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/instant-gratification/train.csv", "test_file": "./data/DSBench/data/data_resplit/instant-gratification/test.csv", "answer_dir": "./data/DSBench/data/answers/instant-gratification", "url": "https://www.kaggle.com/competitions/instant-gratification"}}
{"prompt": "Description  \nAt Santander, our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals. Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan? \n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\nEvaluation  \nSubmissions are evaluated on the area under the ROC curve between the predicted probability and the observed target. \n\nSubmission File  \nFor each Id in the test set, you must make a binary prediction of the target variable. The file should contain a header and have the following format: \n```\nID_code,target\ntest_0,0\ntest_1,1\ntest_2,0\netc.\n```\n\nDataset Description\nYou are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of the target column in the test set.\n\nFile descriptions  \n- train.csv - the training set.\n- test.csv - the test set. The test set contains some rows which are not included in scoring.\n- sample_submission.csv - a sample submission file in the correct format.", "answer": null, "metadata": {"task_id": "santander-customer-transaction-prediction", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/santander-customer-transaction-prediction/train.csv", "test_file": "./data/DSBench/data/data_resplit/santander-customer-transaction-prediction/test.csv", "answer_dir": "./data/DSBench/data/answers/santander-customer-transaction-prediction", "url": "https://www.kaggle.com/competitions/santander-customer-transaction-prediction"}}
{"prompt": "Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!\n\nWith the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n\nSynthetically-Generated Datasets \nUsing synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and the goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve!\n\nEvaluation \nSubmissions are scored on the log loss:\n\\[ \\text{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\]\n\nwhere  \n\\( n \\) is the number of rows in the test set  \n\\( \\hat{y}_i \\) is the predicted probability the Class is a pulsar  \n\\( y_i \\) is 1 if Class is pulsar, otherwise 0  \n\\( \\log \\) is the natural logarithm  \n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.\n\nSubmission File \nFor each id in the test set, you must predict the value for the target Class. The file should contain a header and have the following format:\n```\nid,Class\n117564,0.11\n117565,0.32\n117566,0.95\netc.\n```\n\nDataset Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles  \n- train.csv - the training dataset; Class is the (binary) target  \n- test.csv - the test dataset; your objective is to predict the probability of Class (whether the observation is a pulsar)  \n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "playground-series-s3e10", "task_type": "data_modeling", "data_type": "classification", "train_file": "./data/DSBench/data/data_resplit/playground-series-s3e10/train.csv", "test_file": "./data/DSBench/data/data_resplit/playground-series-s3e10/test.csv", "answer_dir": "./data/DSBench/data/answers/playground-series-s3e10", "url": "https://www.kaggle.com/competitions/playground-series-s3e10"}}
{"prompt": "Description  \nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions, and thus more beginner-friendly.\n\nIn order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching a month-long tabular Playground competition on the 1st of every month, and continue the experiment as long as there's sufficient interest and participation.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals. Good luck and have fun!\n\nGetting Started  \nCheck out this Starter Notebook which walks you through how to make your very first submission! For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.\n\nEvaluation  \nSubmissions are scored on the root mean squared error (RMSE). RMSE is defined as:\n\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\n\nwhere \\( \\hat{y} \\) is the predicted value, \\( y \\) is the original value, and \\( n \\) is the number of rows in the test data.\n\nSubmission File  \nFor each row in the test set, you must predict the value of the target as described on the data tab, each on a separate row in the submission file. The file should contain a header and have the following format:\n\n```\nid,target\n0,0.5\n2,10.2\n6,2.2\netc.\n```\n\nDataset Description  \nFor this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns (cont1 - cont14) are continuous.\n\nFiles  \n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file\n- sample_submission.csv - a sample submission file in the correct format", "answer": null, "metadata": {"task_id": "tabular-playground-series-jan-2021", "task_type": "data_modeling", "data_type": "regression", "train_file": "./data/DSBench/data/data_resplit/tabular-playground-series-jan-2021/train.csv", "test_file": "./data/DSBench/data/data_resplit/tabular-playground-series-jan-2021/test.csv", "answer_dir": "./data/DSBench/data/answers/tabular-playground-series-jan-2021", "url": "https://www.kaggle.com/competitions/tabular-playground-series-jan-2021"}}
