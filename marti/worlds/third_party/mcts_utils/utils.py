import math
from dataclasses import dataclass

from marti.worlds.third_party.mcts_utils.ab_mcts.llm_generation_interface import GenerationRequest, GenerationResult
from marti.worlds.third_party.mcts_utils.ab_mcts.eval_result import EvalResult
from marti.worlds.third_party.mcts_utils.ab_mcts.tasks.base import Task
from marti.worlds.third_party.mcts_utils.ab_mcts.prompts.base import PromptTemplate

import json
import ray
import time
import dataclasses
from vllm import SamplingParams
from pathlib import Path
import datetime
import numpy as np
from typing import List, Optional

from marti.helpers.logging import init_logger
# from marti.worlds.third_party.mcts_utils.run import generate_for_single_mcts


@dataclass
class NodeState:
    generation_result: GenerationResult
    eval_results: EvalResult
    model_name: str

def is_power_of_two(n: int):
    return n > 0 and (n & (n - 1)) == 0

def get_private_score(task: Task, node_state: NodeState | None) -> float:
    if node_state is not None:
        eval_results, _score = task.evaluate_on_test(
            llm_answer=node_state.generation_result
        )
        if len(eval_results) == 0:
            private_score = 0
        else:
            private_score = sum(
                [eval_result.get_score() for eval_result in eval_results]
            ) / len(eval_results)
    else:
        private_score = 0
    return private_score

def process_node(node, task):
    """Helper function to process a single node and calculate scores."""
    if node.expand_idx < 0:
        return None

    node_idx = node.expand_idx
    public_score = node.score

    # calc private score
    node_state = node.state
    private_score = get_private_score(task, node_state)
    # private_score = public_score

    return node_idx, public_score, private_score

def apply_template_with_tokenizer(tokenizer, prompt):
    if isinstance(prompt, str):
        message = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        message = prompt
    return tokenizer.apply_chat_template(
        message,
        tokenize=False,
        add_generation_prompt=True,
    )

async def generate_fn_async(
    state: NodeState | None,
    task: Task,
    prompt_template: PromptTemplate,
    model_name: str,    
    llm,
    sampling_params: dict,
    llm_log_dir: Path,
    tokenizer: None,
    is_eval: bool = False,
    # tokenizer_fn: None,
) -> tuple[NodeState, float]:
    # global total_cost, cost_by_model, time_by_model, node_times

    start_time = time.time()

    # From root
    if state is None:
        messages = [{"role": "user", "content": prompt_template.initial_prompt()}]
    else:
        feedback_prompt = prompt_template.feedback_prompt(
            "transform",
            eval_results=state.eval_results,
            generation_result=state.generation_result,
        )
        messages = [
            {"role": "user", "content": feedback_prompt},
        ]
    
    messages = apply_template_with_tokenizer(tokenizer, messages)

    assert isinstance(messages, str), f"messages must be str"
    assert isinstance(sampling_params, SamplingParams)

    answer = await llm.generate_async.remote(
        prompts=messages,
        sampling_params=sampling_params,
    )
    generation = answer.outputs[0].text
    # cost = len(generation)
    # cost = 0.0
    # Update cost info
    # total_cost += cost
    # if model_name not in cost_by_model:
    #     cost_by_model[model_name] = 0.0
    # cost_by_model[model_name] += cost

    result = GenerationResult(
        request=GenerationRequest(messages=messages), generation=generation
    )

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")[
        :-3
    ]  # up to milliseconds

    log_txt = llm_log_dir / f"log_{timestamp}_{model_name}.txt"
    log_txt.write_text(
        json.dumps(
            {"model": model_name, "result": dataclasses.asdict(result)},
            indent=4,
        )
    )  # save cost and result

    eval_results = task.generate_eval_results(llm_answer=result, kind="transform")
    if eval_results is None:
        score = 0.0
    else:
        score = sum([eval_result.get_score() for eval_result in eval_results]) / len(
            eval_results
        )

    # Calculate execution time for this node
    # execution_time = time.time() - start_time
    # node_times.append(execution_time)

    # # Update time by model
    # if model_name not in time_by_model:
    #     time_by_model[model_name] = 0.0
    # time_by_model[model_name] += execution_time

    return NodeState(
        generation_result=result, eval_results=eval_results, model_name=model_name
    ), score

def generate_fn(
    state: NodeState | None,
    task: Task,
    prompt_template: PromptTemplate,
    model_name: str,    
    llm,
    sampling_params: dict,
    llm_log_dir: Path,
    tokenizer: None,
    is_eval: bool = False,
) -> tuple[NodeState, float]:
    # global total_cost, cost_by_model, time_by_model, node_times

    start_time = time.time()

    # From root
    if state is None:
        messages = [{"role": "user", "content": prompt_template.initial_prompt()}]
    else:
        feedback_prompt = prompt_template.feedback_prompt(
            "transform",
            eval_results=state.eval_results,
            generation_result=state.generation_result,
        )
        messages = [
            {"role": "user", "content": feedback_prompt},
        ]
    
    messages = apply_template_with_tokenizer(tokenizer, messages)


    # assert prompt_token_ids is not None
    assert isinstance(messages, str), f"messages must be str"
    assert isinstance(sampling_params, SamplingParams)
    answer = ray.get(llm.generate_async.remote(
        prompts=messages,
        sampling_params=sampling_params,
    ))

    # generation = answer[0].outputs[0].text
    generation = answer.outputs[0].text
    # cost = 0.0
    # Update cost info
    # total_cost += cost
    # if model_name not in cost_by_model:
    #     cost_by_model[model_name] = 0.0
    # cost_by_model[model_name] += cost

    result = GenerationResult(
        request=GenerationRequest(messages=messages), generation=generation
    )

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")[
        :-3
    ]  # up to milliseconds

    log_txt = llm_log_dir / f"log_{timestamp}_{model_name}.txt"
    log_txt.write_text(
        json.dumps(
            {"model": model_name, "result": dataclasses.asdict(result)},
            indent=4,
        )
    )  # save cost and result

    eval_results = task.generate_eval_results(llm_answer=result, kind="transform")
    if eval_results is None:
        score = 0.0
    else:
        score = sum([eval_result.get_score() for eval_result in eval_results]) / len(
            eval_results
        )

    # Calculate execution time for this node
    # execution_time = time.time() - start_time
    # node_times.append(execution_time)

    # Update time by model
    # if model_name not in time_by_model:
        # time_by_model[model_name] = 0.0
    # time_by_model[model_name] += execution_time

    return NodeState(
        generation_result=result, eval_results=eval_results, model_name=model_name
    ), score

def get_coverage_and_passk(node_list, problem, workflow_args, checkpoint_path=None, prompt_id=0) -> List[float]:
    n_jobs = workflow_args.get("eval_n_jobs", 8)
    # n_jobs = min(8, os.cpu_count())  # Limit to number of CPU cores
    top_k = workflow_args.get("top_k", 1)
    proc_ret_path = Path(checkpoint_path.as_posix().replace(".pkl", "_proc_result.json"))
    # Parallel processing of nodes
    # results = Parallel(n_jobs=n_jobs, prefer="processes")(
    #     delayed(process_node)(node, code_problem)
    #     for node in tqdm(valid_nodes, desc=f"Processing {prompt_id}")
    # )
    results = [
        process_node(node, problem)
        for node in node_list
    ]

    # Filter out None results (from nodes with expand_idx < 0, though already filtered)
    results = [r for r in results if r is not None]

    # Sort results by node_idx (the first element of each tuple in results)
    # This ensures that node_idx_list, public_scores, and private_scores are ordered by node_idx
    results.sort(key=lambda x: x[0])

    # Unpack sorted results
    node_idx_list = []
    public_scores = []
    private_scores = []
    for result in results:
        node_idx, public_score, private_score = (
            result  # No need to check for None here, already filtered
        )
        node_idx_list.append(node_idx)
        public_scores.append(public_score)
        private_scores.append(private_score)
    assert len(node_idx_list) == len(public_scores) == len(private_scores)
    # assert public_scores != private_scores, "public_scores and private_scores should not be the same"
    coverage_final_reward = 0
    for node_id in range(len(node_idx_list)):
        coverage_final_reward = max(private_scores[node_id], coverage_final_reward)

    # transform NumPy array
    public_arr = np.array(public_scores)
    private_arr = np.array(private_scores)

    # get top_k index_list 
    topk_idx = np.argsort(-public_arr)[:top_k]

    # select private score
    selected_private = private_arr[topk_idx]

    # get best pass@k
    best_final_reward = selected_private.max()

    # save node scores info 
    proc_ret = {
        "node_idx_list": node_idx_list,
        "public_scores": public_scores,
        "private_scores": private_scores,
    }
    with open(proc_ret_path, "w") as f:
        json.dump(proc_ret, f)
    # logger.info(f"the coverage_final_reward of {prompt_id}-th prompt: {coverage_final_reward}")
    # logger.info(f"the best_final_reward of {prompt_id}-th prompt: {best_final_reward}")
    return [coverage_final_reward, best_final_reward]
