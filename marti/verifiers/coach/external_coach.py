"""
Coach Interface: Provides evaluation and feedback for agent actions

Coach purpose:
1. Current: Returns scalar reward for RL training
2. Future: Returns feedback + rewritten reasoning chain for SFT

Supported implementations:
- SimpleScalarCoach: Single LLM call, returns score
- RewritingCoach: Analysis + rewrite reasoning chain (future)
- AutonomousCoach: Autonomous agent with tool usage (future)
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
import os
from dotenv import load_dotenv

# Weave import (optional dependency)
try:
    import weave
    WEAVE_AVAILABLE = True
except ImportError:
    WEAVE_AVAILABLE = False


@dataclass
class CoachFeedback:
    """
    Coach evaluation result (extensible data structure)

    Current usage:
        - reward: scalar reward (for RL training) - process quality score
        - outcome_score: continuous reward (result quality, 0.0-1.0)
          * For DSBench: Normalized metric (RMSE, ROC-AUC, etc.)
          * For legacy: Can be binary 0.0 or 1.0 for backwards compatibility

    Future extensions:
        - feedback_text: text feedback
        - rewritten_output: rewritten output (for SFT)
        - error_analysis: error analysis
    """
    # Core fields (required)
    reward: float  # Reward between 0.0-10.0 (process quality score)

    # Continuous result quality score (optional, used for ground truth evaluation)
    outcome_score: Optional[float] = None  # 0.0-1.0, normalized metric reward

    # Extension fields (optional, for future use)
    feedback_text: Optional[str] = None
    rewritten_output: Optional[str] = None
    rewritten_thinking: Optional[str] = None
    error_analysis: Optional[Dict] = None

    # Metadata
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validate that reward and outcome_score are within valid ranges"""
        if not 0.0 <= self.reward <= 10.0:
            raise ValueError(f"Reward must be in [0.0, 10.0], got {self.reward}")
        if self.outcome_score is not None and not (0.0 <= self.outcome_score <= 1.0):
            raise ValueError(f"outcome_score must be in [0.0, 1.0] or None, got {self.outcome_score}")


class BaseCoach(ABC):
    """
    Abstract base class for Coach

    All coach implementations must inherit from this class and implement the evaluate method
    """

    @abstractmethod
    async def evaluate(
        self,
        problem: str,
        agent_input: str,
        agent_output: str,
        agent_role: str,
        context: Optional[Dict[str, Any]] = None
    ) -> CoachFeedback:
        """
        Evaluate an agent action

        Args:
            problem: Original problem/task description
            agent_input: Complete input received by agent (may include chat template)
            agent_output: Output generated by agent (may include <think> tags)
            agent_role: Role of the agent ("ideation", "experimentation", "writeup", etc.)
            context: Additional context information (e.g., outputs from previous agents)

        Returns:
            CoachFeedback object
        """
        pass


def _weave_op_if_available(func):
    """Decorator to apply weave.op if weave is available"""
    if WEAVE_AVAILABLE:
        return weave.op()(func)
    return func

class SimpleScalarCoach(BaseCoach):
    """
    Simple version: Single LLM API call, returns scalar reward

    Supported models (provider auto-inferred):
    - Gemini: "gemini-1.5-pro", "gemini-2.0-flash-thinking-exp", etc.
    - GPT: "gpt-4", "gpt-4o", "gpt-4o-mini", etc.
    - Claude: "claude-3-5-sonnet-20241022", "claude-3-opus", etc.
    - DeepSeek: "deepseek-chat", "deepseek-reasoner", etc. (future)

    Supports both Vertex AI and direct API methods (for Gemini)
    """

    def __init__(
        self,
        model: str,
        prompt_template: str,  # REQUIRED: must be defined in config
        temperature: float = 0.0,
        max_retries: int = 300,  # ~1 day with 5-min capped backoff (300 retries × 5 min = 25 hours)
        fallback_score: float = 5.0,  # Neutral fallback score (midpoint of 0-10 range)
        max_output_tokens: Optional[int] = None,  # None = unlimited
        thinking_budget: Optional[int] = None,  # None = let model decide; 0 = disable thinking
        env_path: Optional[str] = None,
        use_vertex_ai: bool = False,  # Whether to use Vertex AI (Gemini only)
        vertex_project: Optional[str] = None,  # GCP project ID (required for Vertex AI)
        vertex_location: str = "us-central1"  # Vertex AI region
    ):
        """
        Args:
            model: Model name (provider auto-inferred from name)
            prompt_template: Prompt template for coach evaluation (REQUIRED, must be defined in config)
            temperature: Generation temperature (default 0.0, deterministic scoring)
            max_retries: Maximum retry attempts on API failure (default 300, ~1 day)
            fallback_score: Fallback score on API or parsing failure (default 5.0, midpoint of 0-10)
            max_output_tokens: Maximum output tokens (None=unlimited, let model complete naturally)
            thinking_budget: Thinking token budget (None=model decides, 0=disable thinking, positive=limit thinking tokens)
            env_path: Path to .env file (None searches from repo root)
            use_vertex_ai: Whether to use Vertex AI (only applies to Gemini)
            vertex_project: GCP project ID (required when use_vertex_ai=True)
            vertex_location: Vertex AI region
        """
        # Validate prompt_template
        if not prompt_template or not isinstance(prompt_template, str):
            raise ValueError(
                "prompt_template is required and must be a non-empty string.\n"
                "Please define coach_prompt_template in your config file (e.g., mathchat_with_coach.yaml).\n"
                "See the config file for examples of how to define templates."
            )

        self.model = model
        self.temperature = temperature
        self.max_retries = max_retries
        self.fallback_score = fallback_score
        self.max_output_tokens = max_output_tokens
        self.thinking_budget = thinking_budget
        self.use_vertex_ai = use_vertex_ai
        self.vertex_project = vertex_project
        self.vertex_location = vertex_location
        self.prompt_template = prompt_template

        # Infer provider from model name
        self.provider = self._infer_provider(model)

        # Vertex AI uses ADC (Application Default Credentials), no API key needed
        if self.use_vertex_ai and self.provider == "google":
            if not self.vertex_project:
                raise ValueError("vertex_project is required when use_vertex_ai=True")
            self.api_key = None  # Vertex AI uses ADC, not API key
            print(f"[SimpleScalarCoach] Using Vertex AI (ADC auth): {vertex_project}/{vertex_location}")
        else:
            # Load API key (required for non-Vertex AI mode)
            self.api_key = self._load_api_key(env_path)

        # Report all parameters
        print(f"[SimpleScalarCoach] Initialized:")
        print(f"  Provider: {self.provider}")
        print(f"  Model: {self.model}")
        if self.use_vertex_ai:
            print(f"  Auth: Vertex AI ADC (project={self.vertex_project}, location={self.vertex_location})")
        else:
            print(f"  Auth: API Key")
        print(f"  Temperature: {self.temperature}")
        print(f"  Max output tokens: {self.max_output_tokens if self.max_output_tokens else 'None (unlimited)'}")
        thinking_desc = "None (model decides)" if self.thinking_budget is None else (
            "0 (disabled)" if self.thinking_budget == 0 else f"{self.thinking_budget} tokens"
        )
        print(f"  Thinking budget: {thinking_desc}")
        print(f"  Max retries: {self.max_retries} (~{self.max_retries * 5 // 60} hours with 5-min cap)")
        print(f"  Fallback score: {self.fallback_score}")
        print(f"  Prompt template: Defined ({len(self.prompt_template)} chars)")

    def _infer_provider(self, model: str) -> str:
        """
        Infer provider from model name

        Examples:
            "gpt-4" → "openai"
            "gemini-1.5-pro" → "google"
            "claude-3-5-sonnet" → "anthropic"
        """
        model_lower = model.lower()

        # OpenAI models
        if any(prefix in model_lower for prefix in ["gpt-", "o1-", "text-davinci"]):
            return "openai"

        # Anthropic models
        elif any(prefix in model_lower for prefix in ["claude-"]):
            return "anthropic"

        # Google models
        elif any(prefix in model_lower for prefix in ["gemini-", "palm-", "bison-"]):
            return "google"

        # DeepSeek models (future support)
        elif any(prefix in model_lower for prefix in ["deepseek-"]):
            return "deepseek"

        else:
            raise ValueError(
                f"Cannot infer provider from model name: {model}\n"
                f"Supported prefixes: gpt-, claude-, gemini-, deepseek-"
            )

    def _load_api_key(self, env_path: Optional[str] = None) -> str:
        """
        Load API key from .env file

        Search order:
        1. If env_path is provided, use the specified path
        2. Otherwise search upward from current file to find repo root (containing .git folder)
        3. Read from .env in root directory

        For Google, supports two key names:
        - GEMINI_API_KEY (direct Gemini API)
        - GOOGLE_API_KEY (fallback)
        """
        # Try to load .env file if env_path provided or can be found
        if env_path is None:
            # Find repo root directory
            current_file = os.path.abspath(__file__)
            repo_root = os.path.dirname(current_file)

            # Search upward until .git folder is found
            max_depth = 10
            found_git = False
            for _ in range(max_depth):
                if os.path.exists(os.path.join(repo_root, '.git')):
                    found_git = True
                    break
                parent = os.path.dirname(repo_root)
                if parent == repo_root:  # Reached filesystem root
                    break
                repo_root = parent

            if found_git:
                env_path = os.path.join(repo_root, '.env')
                if os.path.exists(env_path):
                    load_dotenv(env_path)
            # If .git not found, skip dotenv (will use env vars directly)
        elif os.path.exists(env_path):
            load_dotenv(env_path)

        # Read API key for the corresponding provider
        if self.provider == "google":
            # Prefer GEMINI_API_KEY, fallback to GOOGLE_API_KEY
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
            key_names = "GEMINI_API_KEY or GOOGLE_API_KEY"
        elif self.provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            key_names = "OPENAI_API_KEY"
        elif self.provider == "anthropic":
            api_key = os.getenv("ANTHROPIC_API_KEY")
            key_names = "ANTHROPIC_API_KEY"
        elif self.provider == "deepseek":
            api_key = os.getenv("DEEPSEEK_API_KEY")
            key_names = "DEEPSEEK_API_KEY"
        else:
            raise ValueError(f"Unknown provider: {self.provider}")

        if not api_key or api_key.strip() == "":
            raise ValueError(
                f"{key_names} not found in {env_path}\n"
                f"Please add it to your .env file"
            )

        return api_key.strip()

    def _create_judge_prompt(
        self,
        problem: str,
        agent_input: str,
        agent_output: str,
        agent_role: str,
        context: Optional[Dict] = None,
        ground_truth: Optional[str] = None
    ) -> str:
        """
        Create evaluation prompt for coach

        Simple approach:
        1. All parameters and context can be used as template variables
        2. Template uses whatever variables it references
        3. If a referenced variable doesn't exist, raise error with list of available variables
        """
        context = context or {}

        # All variables available to template = function parameters + context content
        template_vars = {
            "problem": problem,
            "agent_input": agent_input,
            "agent_output": agent_output,
            "agent_role": agent_role,
            "ground_truth_answer": ground_truth if ground_truth else "N/A",  # NEW
        }

        # Add all context content as variables
        template_vars.update(context)

        # Fill template (always defined, validated in __init__)
        try:
            prompt = self.prompt_template.format(**template_vars)
        except KeyError as e:
            raise ValueError(
                f"Template references undefined variable: {e}\n"
                f"Available variables: {list(template_vars.keys())}"
            )

        return prompt

    def _create_retry_prompt(
        self,
        original_prompt: str,
        original_response: str,
        expected_format: str,
        error_reason: str
    ) -> str:
        """
        Create retry prompt asking LLM to re-output in correct format

        Args:
            original_prompt: Original evaluation prompt
            original_response: LLM's original response (incorrect format)
            expected_format: Expected output format
            error_reason: Reason for format error

        Returns:
            Retry prompt
        """
        retry_prompt = f"""You previously provided this evaluation:

{original_response}

However, the response format was incorrect. Error: {error_reason}

Please provide your evaluation again using EXACTLY this format:

{expected_format}

Do not include any other text or explanation. Only output the score(s) in the exact format shown above.
"""
        return retry_prompt

    @_weave_op_if_available
    async def evaluate(
        self,
        problem: str,
        agent_input: str,
        agent_output: str,
        agent_role: str,
        context: Optional[Dict[str, Any]] = None,
        ground_truth: Optional[str] = None
    ) -> CoachFeedback:
        """
        Evaluate an agent action

        Current implementation: Single API call to get score, retry with format correction on failure

        Args:
            problem: Problem description
            agent_input: Agent's input
            agent_output: Agent's output
            agent_role: Agent role
            context: Additional context
            ground_truth: Ground truth answer (for dual-score evaluation)

        Returns:
            CoachFeedback: Contains reward (process score) and outcome_score (continuous outcome quality)
        """
        # Create evaluation prompt
        judge_prompt = self._create_judge_prompt(
            problem, agent_input, agent_output, agent_role, context, ground_truth
        )

        # Call API to get score and full response (with retry)
        # thought_summary is only returned by Vertex AI (for Weave tracking)
        score, full_response, thought_summary = await self._call_api_with_retry(judge_prompt)

        # Parse response to extract outcome_score (if present)
        parsed_score, outcome_score = self._parse_response(full_response)

        # Check if retry needed (incorrect format)
        needs_retry = False
        retry_reason = []

        # Check 1: Was PROCESS_SCORE successfully extracted
        if parsed_score is None:
            needs_retry = True
            retry_reason.append("PROCESS_SCORE not found in expected format")

        # Check 2: If Verifier with ground truth, must have outcome_score (from OUTCOME_SCORE or ANSWER_CORRECT)
        if ground_truth and ground_truth != "N/A" and agent_role == "Verifier":
            if outcome_score is None:
                needs_retry = True
                retry_reason.append("OUTCOME_SCORE/ANSWER_CORRECT missing for Verifier with ground truth")

        # If retry needed and retries remaining
        max_format_retries = 3  # Maximum 3 retries
        if needs_retry and max_format_retries > 0:
            print(f"[Coach] Format extraction failed: {', '.join(retry_reason)}")
            print(f"[Coach] Original response: {full_response[:200]}...")
            print(f"[Coach] Retrying with format correction prompt...")

            # Construct retry prompt (accepts either OUTCOME_SCORE or ANSWER_CORRECT)
            retry_prompt = self._create_retry_prompt(
                original_prompt=judge_prompt,
                original_response=full_response,
                expected_format="PROCESS_SCORE: [0.0 to 10.0]" +
                               ("\nANSWER_CORRECT: [0 or 1]" if ground_truth and ground_truth != "N/A" else ""),
                error_reason=", ".join(retry_reason)
            )

            # Retry API call
            score_retry, response_retry, _ = await self._call_api_with_retry(retry_prompt)
            parsed_score_retry, outcome_score_retry = self._parse_response(response_retry)

            # If retry successful, use retry results
            if parsed_score_retry is not None:
                score = score_retry
                parsed_score = parsed_score_retry
                full_response = response_retry
                print(f"[Coach] Retry successful: PROCESS_SCORE={parsed_score}")

            if outcome_score_retry is not None:
                outcome_score = outcome_score_retry
                print(f"[Coach] Retry successful: OUTCOME_SCORE={outcome_score}")

        # Final fallback: If parsed_score is still None, use score from _call_api_with_retry
        if parsed_score is None:
            print(f"[Coach] Warning: Using fallback score from old parser: {score}")
            parsed_score = score

        # Return CoachFeedback (thought_summary included in metadata for Weave tracking)
        metadata = {
            "provider": self.provider,
            "model": self.model,
            "agent_role": agent_role,
            "retry_attempted": needs_retry
        }
        if thought_summary:
            metadata["thought_summary"] = thought_summary  # For Weave tracking

        return CoachFeedback(
            reward=parsed_score,
            outcome_score=outcome_score,  # Continuous result quality score (if present)
            feedback_text=full_response,  # Save full response
            metadata={
                **(metadata or {}),  # Include any existing metadata
                "provider": self.provider,
                "model": self.model,
                "agent_role": agent_role,
                "retry_attempted": needs_retry,
                "judge_prompt": judge_prompt  # Save prompt for debug logging
            }
        )

    async def _call_api_with_retry(self, prompt: str) -> tuple[float, str, Optional[str]]:
        """
        Call API and handle retry logic

        Returns:
            (score, full_response, thought_summary): Score, full response text, thought summary (Vertex AI only)
        """
        import asyncio

        for attempt in range(self.max_retries):
            try:
                # Call corresponding API based on provider
                thought_summary = None  # Only Vertex AI returns thought summaries
                if self.provider == "google":
                    if self.use_vertex_ai:
                        score, text, thought_summary = await self._call_vertex_ai(prompt)
                    else:
                        score, text = await self._call_gemini_direct(prompt)
                elif self.provider == "openai":
                    score, text = await self._call_openai(prompt)
                elif self.provider == "anthropic":
                    score, text = await self._call_anthropic(prompt)
                else:
                    raise ValueError(f"Unsupported provider: {self.provider}")

                return score, text, thought_summary

            except Exception as e:
                if attempt < self.max_retries - 1:
                    wait_time = min(2 ** attempt, 300)  # Exponential backoff, max 5 minutes
                    print(f"[Coach] API call failed (attempt {attempt+1}/{self.max_retries}), retrying in {wait_time}s: {e}")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"[Coach] API call failed after {self.max_retries} attempts: {e}")
                    return self.fallback_score, f"Error: {str(e)}", None  # Use fallback score

    async def _call_vertex_ai(self, prompt: str) -> tuple[float, str, Optional[str]]:
        """
        Call Gemini model through Vertex AI (using google-genai SDK)

        Authentication: Uses ADC (Application Default Credentials).
        Setup: Run 'gcloud auth application-default login' once.

        Vertex AI advantages (Gemini 2.0+ models):
        - Dynamic Shared Quota (DSQ): No fixed quota limits
        - Enterprise-grade SLA and support

        Note on thinking models:
        - Response may contain multiple parts: thought summaries (part.thought=True) and answers
        - We extract only the non-thought parts for the actual response
        - Thought summaries are returned separately for Weave tracking
        - thinking_budget is configurable: None=model decides, 0=disabled, positive=limit

        Returns:
            (score, full_text, thought_summary): Score, full response, thought summary (for Weave tracking)
        """
        from google import genai
        from google.genai import types

        # Create client (using Vertex AI endpoint)
        client = genai.Client(
            vertexai=True,
            project=self.vertex_project,
            location=self.vertex_location
        )

        # Build generation config
        gen_config_params = {"temperature": self.temperature}

        # Add thinking_budget if specified (None = don't set, let model decide)
        if self.thinking_budget is not None:
            gen_config_params["thinking_config"] = types.ThinkingConfig(
                thinking_budget=self.thinking_budget
            )

        # Add max_output_tokens if specified
        if self.max_output_tokens is not None:
            gen_config_params["max_output_tokens"] = self.max_output_tokens

        gen_config = types.GenerateContentConfig(**gen_config_params)

        # Generate
        response = await client.aio.models.generate_content(
            model=self.model,
            contents=prompt,
            config=gen_config
        )

        # Extract text from response, handling multiple parts properly
        # Thinking models may return thought summaries (part.thought=True) and answers
        text_parts = []
        thought_parts = []

        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]

            # Check finish reason for debugging
            if hasattr(candidate, 'finish_reason'):
                print(f"[Coach] Finish reason: {candidate.finish_reason}")
                if str(candidate.finish_reason) == "MAX_TOKENS":
                    # Log token usage for debugging
                    if hasattr(response, 'usage_metadata'):
                        usage = response.usage_metadata
                        print(f"[Coach] Token usage - prompt: {getattr(usage, 'prompt_token_count', 'N/A')}, "
                              f"output: {getattr(usage, 'candidates_token_count', 'N/A')}, "
                              f"thoughts: {getattr(usage, 'thoughts_token_count', 'N/A')}")

            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                for part in candidate.content.parts:
                    if hasattr(part, 'text') and part.text:
                        # Check if this is a thought part or answer part
                        is_thought = getattr(part, 'thought', False)
                        if is_thought:
                            thought_parts.append(part.text)
                        else:
                            text_parts.append(part.text)

        # Use answer parts (non-thought), fallback to thought if no answer
        if text_parts:
            text = "\n".join(text_parts).strip()
        elif thought_parts:
            # If only thought parts exist, use them (shouldn't happen normally)
            print(f"[Coach] Warning: Only thought parts in response, using thought summary")
            text = "\n".join(thought_parts).strip()
        else:
            # Try the simple .text accessor as last resort
            try:
                text = response.text.strip()
            except (ValueError, AttributeError):
                raise ValueError(f"No text content in response. Candidates: {response.candidates}")

        # Combine thought parts for Weave tracking
        thought_summary = "\n".join(thought_parts) if thought_parts else None

        # Print full response for debugging
        print(f"\n[Coach] Vertex AI full response:")
        if thought_summary:
            truncated = thought_summary[:200] + "..." if len(thought_summary) > 200 else thought_summary
            print(f"[Thought summary]: {truncated}")
        print(f"[Answer]: {text}")
        print(f"[Coach] " + "-" * 60)

        # Parse score
        score = self._parse_score(text)
        return score, text, thought_summary

    async def _call_gemini_direct(self, prompt: str) -> tuple[float, str]:
        """
        Call Gemini API directly (not through Vertex AI)

        Uses REST API: https://generativelanguage.googleapis.com

        Returns:
            (score, full_text): Score and full response
        """
        import aiohttp
        import json

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent?key={self.api_key}"

        # Build generation config
        gen_config = {"temperature": self.temperature}
        if self.max_output_tokens is not None:
            gen_config["maxOutputTokens"] = self.max_output_tokens

        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": gen_config
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    data = await response.json()

                    # Print full API response for debugging
                    print(f"\n[Coach] Gemini API full response:")
                    print(json.dumps(data, indent=2))
                    print(f"[Coach] " + "-" * 60)

                    # Parse response (handle different response formats)
                    try:
                        # Standard format
                        text = data["candidates"][0]["content"]["parts"][0]["text"]
                    except (KeyError, IndexError, TypeError):
                        # Try to extract text from other locations
                        if "candidates" in data and len(data["candidates"]) > 0:
                            candidate = data["candidates"][0]
                            if "output" in candidate:
                                text = candidate["output"]
                            elif "text" in candidate:
                                text = candidate["text"]
                            else:
                                raise KeyError(f"Cannot find text in response: {candidate.keys()}")
                        else:
                            raise KeyError(f"No candidates in response: {data.keys()}")

                    print(f"\n[Coach] Extracted text:")
                    print(f"{text}")
                    print(f"[Coach] " + "-" * 60)

                    score = self._parse_score(text)
                    return score, text
                else:
                    error_text = await response.text()
                    raise RuntimeError(f"Gemini API error ({response.status}): {error_text}")

    async def _call_openai(self, prompt: str) -> tuple[float, str]:
        """
        Call OpenAI API (async client)

        Returns:
            (score, full_text): Score and full response
        """
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=self.api_key)

        # Build parameters (only pass max_output_tokens if set)
        params = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature
        }
        if self.max_output_tokens is not None:
            params["max_tokens"] = self.max_output_tokens

        response = await client.chat.completions.create(**params)

        text = response.choices[0].message.content.strip()

        # Print full response
        print(f"\n[Coach] OpenAI full response:")
        print(f"{text}")
        print(f"[Coach] " + "-" * 60)

        score = self._parse_score(text)
        return score, text

    async def _call_anthropic(self, prompt: str) -> tuple[float, str]:
        """
        Call Anthropic API (async client)

        Returns:
            (score, full_text): Score and full response
        """
        from anthropic import AsyncAnthropic

        client = AsyncAnthropic(api_key=self.api_key)

        # Build parameters
        # Note: Anthropic API's max_tokens is a required parameter, use large value if None
        params = {
            "model": self.model,
            "temperature": self.temperature,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": self.max_output_tokens if self.max_output_tokens else 4096  # Anthropic requires this parameter
        }

        response = await client.messages.create(**params)

        text = response.content[0].text.strip()

        # Print full response
        print(f"\n[Coach] Anthropic full response:")
        print(f"{text}")
        print(f"[Coach] " + "-" * 60)

        score = self._parse_score(text)
        return score, text

    def _parse_response(self, response_text: str) -> tuple[Optional[float], Optional[float]]:
        """
        Extract score and outcome_score from LLM response (if present)

        Supports multiple formats (backward compatible):
        - PROCESS_SCORE: 0.85
        - OUTCOME_SCORE: 0.75 (continuous 0.0-1.0, for DSBench)
        - ANSWER_CORRECT: 0 or 1 (binary, for MathChat - converted to 0.0/1.0)

        Returns:
            (score, outcome_score): Score and continuous result quality (None if not present)
            Note: score may be None (indicates PROCESS_SCORE format not found), handled by outer layer
        """
        import re

        text = response_text.strip()

        # Try to extract PROCESS_SCORE (new format, case-insensitive)
        score = None
        score_match = re.search(r'PROCESS_SCORE:\s*([\d.]+)', text, re.IGNORECASE)
        if score_match:
            score = float(score_match.group(1))
            score = max(0.0, min(10.0, score))  # Clamp to [0, 10]
        # Note: No longer fallback to old parser here, let outer retry mechanism handle

        # Try to extract OUTCOME_SCORE (optional, case-insensitive, continuous 0.0-1.0)
        outcome_score = None
        outcome_match = re.search(r'OUTCOME_SCORE:\s*([\d.]+)', text, re.IGNORECASE)
        if outcome_match:
            outcome_score = float(outcome_match.group(1))
            outcome_score = max(0.0, min(1.0, outcome_score))  # Clamp to [0, 1]

        # Backward compatibility: Also accept ANSWER_CORRECT (binary 0/1 -> float 0.0/1.0)
        # This allows MathChat configs to keep using ANSWER_CORRECT format
        if outcome_score is None:
            answer_match = re.search(r'ANSWER_CORRECT:\s*([01])', text, re.IGNORECASE)
            if answer_match:
                outcome_score = float(answer_match.group(1))  # 0 -> 0.0, 1 -> 1.0

        return score, outcome_score

    def _parse_score(self, response_text: str) -> float:
        """
        Extract numerical score from LLM response (legacy version, backward compatible)

        Supports multiple formats:
        - "0.75"
        - "Score: 0.75"
        - "0.75/1.0"
        - "The score is 0.75"
        """
        import re

        # Remove all whitespace
        text = response_text.strip()

        # Try to match numbers (0.0-10.0 range)
        patterns = [
            r'([0-1]\.\d+)',      # 0.75 (priority for backward compat 0-1 format)
            r'(\d+\.?\d*)',       # Any number (covers 0-10, e.g., "7.5", "6")
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    score = float(match.group(1))
                    # Ensure in [0, 10] range
                    if 0 <= score <= 10:
                        return score
                except ValueError:
                    continue

        # If unable to parse, log warning and return neutral score
        print(f"[Coach] Warning: Cannot parse score from '{response_text}', using fallback {self.fallback_score}")
        return self.fallback_score


def create_coach(
    model: str,
    coach_type: str = "simple",
    **kwargs
) -> BaseCoach:
    """
    Factory function: Create coach instance

    Args:
        model: Model name (provider auto-inferred)
               Examples: "gemini-1.5-pro", "gpt-4o", "claude-3-5-sonnet"
        coach_type: "simple" | "rewriting" | "autonomous"
        **kwargs: Other parameters, including:
                 - prompt_template (str, REQUIRED): Prompt template for coach evaluation
                 - temperature (float): Generation temperature, default 0.0
                 - max_retries (int): Maximum retry attempts, default 300 (~1 day, 5-min intervals)
                 - fallback_score (float): Fallback score on failure, default 5.0
                 - max_output_tokens (Optional[int]): Maximum output tokens, default None (unlimited)
                 - thinking_budget (Optional[int]): Thinking token budget, default None (model decides)
                   - None: Let model decide how much to think
                   - 0: Disable thinking
                   - Positive: Limit thinking token count
                 - use_vertex_ai (bool): Whether to use Vertex AI, default False
                 - vertex_project (Optional[str]): GCP project ID (required for Vertex AI)
                 - vertex_location (str): Vertex AI region, default "global" (required for Gemini 2.5+)

    Returns:
        BaseCoach instance

    Examples:
        # Basic usage (prompt_template required)
        template = "Evaluate: {problem}\\nAgent: {agent_role}\\nOutput: {agent_output}\\nScore:"
        coach = create_coach(model="gemini-2.5-pro", prompt_template=template)

        # Using Vertex AI
        coach = create_coach(
            model="gemini-2.5-pro",
            prompt_template=template,
            use_vertex_ai=True,
            vertex_project="my-gcp-project"
        )

        # Using OpenAI
        coach = create_coach(model="gpt-4o", prompt_template=template)
    """
    if coach_type == "simple":
        return SimpleScalarCoach(model=model, **kwargs)

    elif coach_type == "rewriting":
        # TODO: Implement in Step 7+
        raise NotImplementedError("RewritingCoach will be implemented in future steps")

    elif coach_type == "autonomous":
        # TODO: Implement in Step 7+
        raise NotImplementedError("AutonomousCoach will be implemented in future steps")

    else:
        raise ValueError(f"Unknown coach_type: {coach_type}")
